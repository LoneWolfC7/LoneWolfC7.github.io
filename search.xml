<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>图</title>
    <url>//2025/02/12/graph/</url>
    <content><![CDATA[<h1 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h1><p>图(G)是由顶点集(V)和边集(E)组成的有序对，G&#x3D;(V,E)，可以用|V|，|E|表示图中的顶点数量和边数量<br>图既可以是有向的，即单向链接，也可以是无向的，即双向链接。<br>只包含有向边的图称为有向图，只包含无向边的图称为无向图<br>若边关联了权重&#x2F;成本，则叫做权重图，反之叫做非权重图</p>
<h1 id="图的属性"><a href="#图的属性" class="headerlink" title="图的属性"></a>图的属性</h1><p>一条边的两个端点为同一个顶点，称为自环<br>同一条边出现了不止一次，称为多重边<br>不包含自环或多重边的图称为简单图</p>
<ul>
<li>边的数量<br>在一个简单图中，给定顶点的数量n(|V|&#x3D;n)<br>有向图：0≤|E|≤n*(n-1)<br>无向图：0≤|E|≤n*(n-1)&#x2F;2<br>若图中的边的数量接近最大的可能边数(顶点数量的平方)，则称这个图是稠密的。<br>若图中的边的数量接近顶点数量，则称这个图是稀疏的。<br>对图进行处理时，根据图的稠密&#x2F;稀疏做出决策，如 使用邻接矩阵存储稠密的图，使用邻接表存储稀疏的图</li>
<li>Path,Walk,Trail<br>简单路径(Path)：没有重复边&#x2F;点的途径(Walk)<br>途径中的顶点可以重复但是边不能重复，则称为Trail</li>
<li>链接<br>强链接：从任意顶点可以去到任意的其他顶点<br>若是一个无向图，直接称它为connected</li>
<li>环<br>开始与结束为同一顶点且边数&gt;0</li>
</ul>
<h1 id="图的存储方式"><a href="#图的存储方式" class="headerlink" title="图的存储方式"></a>图的存储方式</h1><p>图的存储方式分为邻接矩阵和邻接表。使用邻接矩阵，当结点数量较多时，比如有1e5个点，那么至少开1e10的数组。<br>相较之下，使用邻接表就更为方便。邻接表包括前向星和vector两种形式。使用vector变长数组支持排序。<br>用vector<int> g[N]来存图，g[i]表示i的所有出点编号。若有x-&gt;y，则g[x].push_back(y)</p>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>DFS与BFS</title>
    <url>//2025/02/17/DFS-and-BFS/</url>
    <content><![CDATA[<h1 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> dep)</span><span class="comment">//dep代表递归层数或要填的第几个空</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(所有空都填完了)</span><br><span class="line">    &#123;</span><br><span class="line">        判断最优解/记录答案；</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(枚举这个空能填的选项)</span><br><span class="line">        <span class="keyword">if</span>(这个选项是合法的)</span><br><span class="line">        &#123;</span><br><span class="line">            记录下这个空（保存现场）;</span><br><span class="line">            <span class="built_in">dfs</span>(dep<span class="number">+1</span>);</span><br><span class="line">            取消这个空（恢复现场）;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bfs</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    queue&lt;<span class="type">int</span>&gt; q;</span><br><span class="line">    q.<span class="built_in">push</span>(初始状态)<span class="comment">//将初始状态入队</span></span><br><span class="line">    <span class="keyword">while</span>(q.<span class="built_in">size</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> u = q.<span class="built_in">front</span>();</span><br><span class="line">        q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">for</span>(枚举所有可扩展状态)<span class="comment">//找到u的所有可达状态v</span></span><br><span class="line">            <span class="keyword">if</span>(是合法的)<span class="comment">//v需要满足某些条件，如未访问过、未在队内等</span></span><br><span class="line">                q.<span class="built_in">push</span>(v);<span class="comment">//入队（同时可能需要维护某些必要信息）</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="树的遍历"><a href="#树的遍历" class="headerlink" title="树的遍历"></a>树的遍历</h1>]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>离散化与树状数组</title>
    <url>//2025/02/21/discretization-and-BIT/</url>
    <content><![CDATA[<h1 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h1><p>特征：操作次数不多，相关点不多</p>
<ul>
<li>离线与在线<br>离线：先存操作&#x2F;询问后执行操作<br>在线：输入一次数据执行一次操作</li>
</ul>
<ol>
<li>找出相关点并存下&#x2F;收集所有需要离散化的值</li>
<li>将相关点排序并去重</li>
<li>大点(小点)到小点(大点)的映射<br>通过二分实现大点映射到小点（根据值找位置-&gt;二分），通过离散化数组实现小点映射到大点</li>
<li>将操作全部转化为小点上</li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*输入：</span></span><br><span class="line"><span class="comment">n 个添加操作，每个操作包含一个坐标 x 和一个权重 w。</span></span><br><span class="line"><span class="comment">q 个查询操作，每个查询包含一个区间 [l, r]。</span></span><br><span class="line"><span class="comment">目标：</span></span><br><span class="line"><span class="comment">对每个查询，计算区间 [l, r] 内所有 x 的权重和。*/</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">3e5</span> + <span class="number">9</span>;</span><br><span class="line"><span class="keyword">using</span> ll = <span class="type">long</span> <span class="type">long</span>;</span><br><span class="line"><span class="type">int</span> a[N];</span><br><span class="line">vector&lt;<span class="type">int</span>&gt; LS;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">op</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a, b;</span><br><span class="line">&#125; add[N], que[N];</span><br><span class="line"><span class="comment">//离散化映射</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getindex</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">lower_bound</span>(LS.<span class="built_in">begin</span>(), LS.<span class="built_in">end</span>(), x) - LS.<span class="built_in">begin</span>() + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="number">0</span>), cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> n, q;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; q;</span><br><span class="line">    <span class="comment">//找出相关点并存下/收集所有需要离散化的值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> w, x;</span><br><span class="line">        cin &gt;&gt; w &gt;&gt; x;</span><br><span class="line">        LS.<span class="built_in">push_back</span>(w);</span><br><span class="line">        add[i] = &#123;w, x&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= q;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> l, r;</span><br><span class="line">        cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">        LS.<span class="built_in">push_back</span>(l), LS.<span class="built_in">push_back</span>(r);</span><br><span class="line">        que[i] = &#123;l, r&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//排序和去重</span></span><br><span class="line">    <span class="built_in">sort</span>(LS.<span class="built_in">begin</span>(), LS.<span class="built_in">end</span>());</span><br><span class="line">    LS.<span class="built_in">erase</span>(<span class="built_in">unique</span>(LS.<span class="built_in">begin</span>(), LS.<span class="built_in">end</span>()), LS.<span class="built_in">end</span>());</span><br><span class="line">    <span class="comment">//将操作全部转化为小点上</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n;i++)</span><br><span class="line">        a[<span class="built_in">getindex</span>(add[i].a)] += add[i].b;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>;i&lt;=LS.<span class="built_in">size</span>();i++)</span><br><span class="line">        a[i] += a[i - <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= q;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> l = <span class="built_in">getindex</span>(que[i].a), r = <span class="built_in">getindex</span>(que[i].b);</span><br><span class="line">        cout &lt;&lt; a[r] - a[l - <span class="number">1</span>]&lt;&lt;<span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="树状数组"><a href="#树状数组" class="headerlink" title="树状数组"></a>树状数组</h1><p>lowbit()的实现，树状数组的初始化</p>
<h2 id="单点修改"><a href="#单点修改" class="headerlink" title="单点修改"></a>单点修改</h2>]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>CS61B学习记录</title>
    <url>//2025/04/21/CS61B-study-notes/</url>
    <content><![CDATA[<h1 id="java基础知识"><a href="#java基础知识" class="headerlink" title="java基础知识"></a>java基础知识</h1><p>在java中，只有八种基本类型（byte,short,int,double,float,long,char,boolean）在调用时是值传递的调用，其他类型均为地址传递（引用）<br>所以，创建一个string类型的数组时，存放的是每个string类型的地址<br>创建一个数组的方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">x = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">3</span>];</span><br><span class="line">y = <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line"><span class="type">int</span> [] z = &#123;<span class="number">9</span>,<span class="number">213</span>,<span class="number">41</span>,<span class="number">12</span>&#125;;</span><br></pre></td></tr></table></figure>
<p>类可以进行嵌套。如SLList里嵌套了IntNode类，IntNode只是SLList的一个子功能<br>泛型实例化不能直接对数组使用，而要通过</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">items = (T[]) <span class="keyword">new</span> <span class="title class_">Object</span>[<span class="number">9</span>];</span><br></pre></td></tr></table></figure>
<p>语句实现</p>
<h1 id="IntList"><a href="#IntList" class="headerlink" title="IntList"></a>IntList</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">IntList</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> first;</span><br><span class="line">    <span class="keyword">public</span> IntList rest;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">IntList</span> <span class="variable">L</span> <span class="operator">=</span><span class="keyword">new</span> <span class="title class_">IntList</span>();</span><br><span class="line">        L.first = <span class="number">5</span>;</span><br><span class="line">        L.rest = <span class="literal">null</span>;</span><br><span class="line">        <span class="comment">//创建第一个新节点</span></span><br><span class="line">        L.rest = <span class="keyword">new</span> <span class="title class_">IntList</span>();</span><br><span class="line">        L.rest.first = <span class="number">15</span>;</span><br><span class="line">        <span class="comment">//创建第二个新节点</span></span><br><span class="line">        L.rest.rest = <span class="keyword">new</span> <span class="title class_">IntList</span>();</span><br><span class="line">        L.rest.first = <span class="number">20</span>;</span><br><span class="line">        L.rest.rest.rest= <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但当结点过多时，每次这样添加结点过于臃肿，所以为其添加构造函数，反向构建列表（尾插法）。并添加了大小和获取结点的方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">IntList</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> first;</span><br><span class="line">    <span class="keyword">public</span> IntList rest;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">IntList</span><span class="params">(<span class="type">int</span> f,IntList r)</span></span><br><span class="line">    &#123;</span><br><span class="line">        first=f;</span><br><span class="line">        rest=r;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">size1</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(rest==<span class="literal">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + rest.size1();</span><br><span class="line">    &#125;<span class="comment">//递归法获取链表大小</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">size2</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">total</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">IntList</span> <span class="variable">p</span> <span class="operator">=</span> <span class="built_in">this</span>;<span class="comment">//Node* temp=head;</span></span><br><span class="line">        <span class="keyword">while</span>(p!=<span class="literal">null</span>)<span class="comment">//while(temp-&gt;next!=nullptr)</span></span><br><span class="line">        &#123;</span><br><span class="line">            total+=<span class="number">1</span>;</span><br><span class="line">            p=p.rest;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> total;</span><br><span class="line">    &#125;<span class="comment">//迭代法获取链表大小</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">get1</span><span class="params">(<span class="type">int</span> i)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(i==<span class="number">0</span>)<span class="keyword">return</span> first;</span><br><span class="line">        <span class="keyword">return</span> rest.get1(i-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">get2</span><span class="params">(<span class="type">int</span> i)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">place</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">IntList</span> <span class="variable">p</span> <span class="operator">=</span> <span class="built_in">this</span>;</span><br><span class="line">        <span class="keyword">while</span>(place!=i)</span><br><span class="line">        &#123;</span><br><span class="line">            place++;</span><br><span class="line">            p=p.rest;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> p.first;</span><br><span class="line">    &#125;<span class="comment">//迭代法获取链表中第i个结点的值</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">IntList</span> <span class="variable">L</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntList</span>(<span class="number">15</span>,<span class="literal">null</span>);</span><br><span class="line">        L = <span class="keyword">new</span> <span class="title class_">IntList</span>(<span class="number">10</span>,L);</span><br><span class="line">        L = <span class="keyword">new</span> <span class="title class_">IntList</span>(<span class="number">5</span>,L);</span><br><span class="line">        System.out.println(L.size1());</span><br><span class="line">        System.out.println(L.size2());</span><br><span class="line">        System.out.println(L.get1(<span class="number">2</span>));</span><br><span class="line">        System.out.println(L.get2(<span class="number">2</span>));</span><br><span class="line">    &#125;<span class="comment">//5-&gt;10-&gt;15</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="SLList-singly-linked-list"><a href="#SLList-singly-linked-list" class="headerlink" title="SLList(singly-linked list)"></a>SLList(singly-linked list)</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SLList</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">class</span> <span class="title class_">IntNode</span> &#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> item;</span><br><span class="line">        <span class="keyword">public</span> IntNode next;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="title function_">IntNode</span><span class="params">(<span class="type">int</span> i, IntNode n)</span> &#123;</span><br><span class="line">            item = i;</span><br><span class="line">            next = n;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> size;</span><br><span class="line">    <span class="keyword">private</span> IntNode first;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">SLList</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">        first = <span class="keyword">new</span> <span class="title class_">IntNode</span>(x, <span class="literal">null</span>);</span><br><span class="line">        size = <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="comment">//构造函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">SLList</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        first = <span class="literal">null</span>;</span><br><span class="line">        size = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addfirst</span><span class="params">(<span class="type">int</span> x)</span></span><br><span class="line">    &#123;</span><br><span class="line">        first = <span class="keyword">new</span> <span class="title class_">IntNode</span>(x,first);</span><br><span class="line">        size++;</span><br><span class="line">    &#125;<span class="comment">//头插法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addlast</span><span class="params">(<span class="type">int</span> x)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">IntNode</span> <span class="variable">p</span> <span class="operator">=</span> first;</span><br><span class="line">        <span class="keyword">while</span>(p!=<span class="literal">null</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            p = p.next;</span><br><span class="line">        &#125;</span><br><span class="line">        p = <span class="keyword">new</span> <span class="title class_">IntNode</span> (x,<span class="literal">null</span>);</span><br><span class="line">        size++;</span><br><span class="line">    &#125;<span class="comment">//尾插法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getfirst</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> first.item;</span><br><span class="line">    &#125;<span class="comment">//得到第一个节点的值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="title function_">size</span><span class="params">(IntNode p)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(p==<span class="literal">null</span>)<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + size(p.next);</span><br><span class="line">    &#125;<span class="comment">//辅助size()</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">size</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> size;</span><br><span class="line"><span class="comment">//        return size(first);</span></span><br><span class="line">    &#125;<span class="comment">//获取链表大小</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">SLList</span> <span class="variable">p</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SLList</span>(<span class="number">3</span>);</span><br><span class="line">        p.addfirst(<span class="number">32</span>);</span><br><span class="line">        System.out.println(p.getfirst());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但当我们对空列表进行尾插时，会发生报错，是因为Null不存在next，可以将特殊情况特判，但当方法很多时会过于臃肿。因此我们需要设置一个虚首节点“哨兵”</p>
<p>现存方法：头插法，尾插法，大小，得到第一个节点的值</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SLList</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">class</span> <span class="title class_">IntNode</span> &#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> item;</span><br><span class="line">        <span class="keyword">public</span> IntNode next;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="title function_">IntNode</span><span class="params">(<span class="type">int</span> i, IntNode n)</span> &#123;</span><br><span class="line">            item = i;</span><br><span class="line">            next = n;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> size;</span><br><span class="line">    <span class="keyword">private</span> IntNode Sentinel;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">SLList</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">        Sentinel = <span class="keyword">new</span> <span class="title class_">IntNode</span>(<span class="number">0</span>,<span class="literal">null</span>);</span><br><span class="line">        Sentinel.next = <span class="keyword">new</span> <span class="title class_">IntNode</span>(x, <span class="literal">null</span>);</span><br><span class="line">        size = <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="comment">//构造函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">SLList</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        Sentinel = <span class="keyword">new</span> <span class="title class_">IntNode</span>(<span class="number">0</span>,<span class="literal">null</span>);</span><br><span class="line">        size = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addfirst</span><span class="params">(<span class="type">int</span> x)</span></span><br><span class="line">    &#123;</span><br><span class="line">        Sentinel.next = <span class="keyword">new</span> <span class="title class_">IntNode</span>(x,Sentinel.next);</span><br><span class="line">        size++;</span><br><span class="line">    &#125;<span class="comment">//头插法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addlast</span><span class="params">(<span class="type">int</span> x)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">IntNode</span> <span class="variable">p</span> <span class="operator">=</span> Sentinel;</span><br><span class="line">        <span class="keyword">while</span>(p.next!=<span class="literal">null</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            p = p.next;</span><br><span class="line">        &#125;</span><br><span class="line">        p.next = <span class="keyword">new</span> <span class="title class_">IntNode</span> (x,<span class="literal">null</span>);</span><br><span class="line">        size++;</span><br><span class="line">    &#125;<span class="comment">//尾插法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getfirst</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> Sentinel.next.item;</span><br><span class="line">    &#125;<span class="comment">//得到第一个节点的值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="title function_">size</span><span class="params">(IntNode p)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(p==<span class="literal">null</span>)<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + size(p.next);</span><br><span class="line">    &#125;<span class="comment">//辅助size()</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">size</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> size;</span><br><span class="line"><span class="comment">//        return size(first);</span></span><br><span class="line">    &#125;<span class="comment">//获取链表大小</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">SLList</span> <span class="variable">p</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SLList</span>(<span class="number">3</span>);</span><br><span class="line">        p.addfirst(<span class="number">32</span>);</span><br><span class="line">        System.out.println(p.getfirst());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="DLList"><a href="#DLList" class="headerlink" title="DLList"></a>DLList</h1><p>上述尾插的时间复杂度为O(n)，为优化为O(1)，我们可以在链表末尾添加一个尾指针。但addlast函数就会存在问题。这时我们构建双向链表，但此时又存在尾指针指向“哨兵”的情况。解决方法有：在末尾再设置一个“哨兵”or将尾指针指向起始“哨兵”</p>
<h1 id="AList"><a href="#AList" class="headerlink" title="AList"></a>AList</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AList</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span>[] items;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> size;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">AList</span><span class="params">()</span> &#123;</span><br><span class="line">        items = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">100</span>];</span><br><span class="line">        size = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addlast</span><span class="params">(<span class="type">int</span> x)</span></span><br><span class="line">    &#123;</span><br><span class="line">        items[size] = x;</span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getlast</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> items[size-<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">get</span><span class="params">(<span class="type">int</span> i)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> items[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">size</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> size;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">removelast</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">x</span> <span class="operator">=</span> getlast();</span><br><span class="line">        size--;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">resize</span><span class="params">(<span class="type">int</span> capacity)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> [] a = <span class="keyword">new</span> <span class="title class_">int</span> [capacity];</span><br><span class="line">        System.arraycopy(items,<span class="number">0</span>,a,<span class="number">0</span>,size);</span><br><span class="line">        items = a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addLast</span><span class="params">(<span class="type">int</span> x)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(size == items.length)resize(size * <span class="number">2</span>);</span><br><span class="line">        items[size]=x;</span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="BST"><a href="#BST" class="headerlink" title="BST"></a>BST</h1><p>二叉搜索树，由有序链表形成，需满足 1.二叉树 2.左边的子节点比本节点小，右边的节点比本节点大</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> BST <span class="title function_">find</span><span class="params">(BST T , Key sk)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(T == <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">if</span>(sk.equals(T.key)) <span class="keyword">return</span> T;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(sk&lt;T.key) <span class="keyword">return</span> find(T.left , sk);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> find(T.right , sk);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">static</span> BST <span class="title function_">insert</span><span class="params">(BST T , Key ik)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(T == <span class="literal">null</span>) <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">BST</span>(ik);</span><br><span class="line">    <span class="keyword">if</span>(ik &lt; T.key) T.left = insert(T.left , ik);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(ik &gt; T.key) T.right = insert(T.right , ik);</span><br><span class="line">    <span class="keyword">return</span> T;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>删除节点时，若无子节点直接取消引用，有一个子节点直接父节点指向子节点，有两个节点时，将所有子节点中最大&#x2F;最小的节点作为根节点</p>
]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>排序二分位运算双指针</title>
    <url>//2025/05/06/sort-binary-search-bitwise-two-pointers/</url>
    <content><![CDATA[<h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><h2 id="去重排序"><a href="#去重排序" class="headerlink" title="去重排序"></a>去重排序</h2><p>vector + sort + unique</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sort</span>(a.<span class="built_in">begin</span>(), a.<span class="built_in">end</span>());</span><br><span class="line">a.<span class="built_in">erase</span>((a.<span class="built_in">begin</span>(), a.<span class="built_in">end</span>()), a.end);</span><br></pre></td></tr></table></figure>
<h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><p>当需要排序的值有许多重复值时，即数组大小远大于数据的大小时</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">&#123;</span><br><span class="line">    cin &gt;&gt; x;</span><br><span class="line">    a[x]++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; a[i]; j++)</span><br><span class="line">        cout &lt;&lt; i &lt;&lt; <span class="string">&#x27; &#x27;</span>;</span><br></pre></td></tr></table></figure>
<h2 id="结构体排序"><a href="#结构体排序" class="headerlink" title="结构体排序"></a>结构体排序</h2><p>当一个元素有多个值且明确要求有先后顺序时<br>在结构体里重载比较运算符，自定义sort</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">book</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a, b, c;</span><br><span class="line">    <span class="type">bool</span> <span class="keyword">operator</span>&gt;(<span class="type">const</span> book &amp;v) <span class="type">const</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (a == v.a &amp;&amp; b == v.b)</span><br><span class="line">            <span class="keyword">return</span> c &gt; v.c;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a == v.a)</span><br><span class="line">            <span class="keyword">return</span> b &gt; v.b;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> a &gt; v.a;</span><br><span class="line">    &#125; <span class="comment">// 按a,b,c排序</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">sort</span>(a + <span class="number">1</span>, a + <span class="number">1</span> + n, <span class="built_in">greater</span>&lt;book&gt;()); <span class="comment">// 降序</span></span><br></pre></td></tr></table></figure>

<hr>
<h1 id="二分"><a href="#二分" class="headerlink" title="二分"></a>二分</h1><h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//先确定l,r和check()函数，注意l,r的选取确保不会出现死循环</span></span><br><span class="line">    <span class="keyword">while</span> (l + <span class="number">1</span> != r)</span><br><span class="line">    &#123;</span><br><span class="line">        mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>())</span><br><span class="line">            l = mid; <span class="comment">// r = mid</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            r = mid;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="二分答案"><a href="#二分答案" class="headerlink" title="二分答案"></a>二分答案</h2><hr>
<h1 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h1><h2 id="基本运算符"><a href="#基本运算符" class="headerlink" title="基本运算符"></a>基本运算符</h2><ul>
<li>&amp;</li>
</ul>
<p>两个位都为1时，结果位为1，否则为0</p>
<p>做&amp;运算时结果不会变大</p>
<ul>
<li>|</li>
</ul>
<p>两个位有一个为1时，结果位为1，否则为0</p>
<p>做|运算时结果不会变小</p>
<ul>
<li>^</li>
</ul>
<p>两个位不相同时，结果位为1，否则为0<br>x ^ y&#x3D; y^ x<br>(x ^ y) ^ z &#x3D; x ^ ( y ^ z)<br>x ^ x &#x3D; 0<br>x ^ 0 &#x3D; x<br>x ^ y &#x3D; z  &#x3D;&gt;  z ^ y &#x3D; x</p>
<ul>
<li>~</li>
</ul>
<p>0-&gt;1 , 1-&gt;0（非符号位）</p>
<ul>
<li>&lt;&lt;(&gt;&gt;)</li>
</ul>
<p>将一个数的二进制向左&#x2F;高位移动指定位数(不带符号位)，低位补0</p>
<p>左移操作&lt;&#x3D;&gt;对原数进行乘2的幂次方 – 13&lt;&lt;3 &#x3D;&#x3D; 13 * 2 *2 *2</p>
<p>（将一个数的二进制向右&#x2F;低位移动指定位数(不带符号位)，高位补0</p>
<p>右移操作&lt;&#x3D;&gt;对原数除以2的幂次方）</p>
<h2 id="运算操作"><a href="#运算操作" class="headerlink" title="运算操作"></a>运算操作</h2><ol>
<li>判断数字奇偶性<br>x &amp; 1 &#x3D; 1    &#x3D;&gt;  奇数<br>x &amp; 1 &#x3D; 0    &#x3D;&gt;  偶数</li>
<li>获取二进制的第i位<br>x&gt;&gt;i&amp;1</li>
<li>修改二进制的第i位为1&#x2F;0<br>x | (1&lt;&lt;i)    1<br>x &amp; ~(1&lt;&lt;i)    0</li>
<li>判断一个数是否为2的幂次方<br>x &amp; (x-1)  结果为0，则是</li>
<li>获取二进制中最低位的1<br>lowbit(x) &#x3D; x &amp; (-x)<br>若x &#x3D; (010010),则lowbit(x) &#x3D; (000010)</li>
</ol>
<h1 id="双指针"><a href="#双指针" class="headerlink" title="双指针"></a>双指针</h1>]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Jupyter的缩写启动方式</title>
    <url>//2025/08/15/Jupyter-shortcut-launch/</url>
    <content><![CDATA[<h1 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h1><p>找到Python Scripts文件夹（jupyter.exe 一般在这里）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Program Files\Python310\Scripts</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Users\你的用户名\AppData\Local\Programs\Python\Python310\Scripts</span><br></pre></td></tr></table></figure>
<p>需要确保的是，这个文件夹在系统环境变量PATH里</p>
<p>在该目录下创建缩写.bat(下以jnb.bat为例)，内容是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@echo off</span><br><span class="line">jupyter notebook %*</span><br></pre></td></tr></table></figure>

<p>随后启动jupyter notebook时直接输入jnb即可。</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>原理也很简单，执行bat里的命令时，默认会把要执行的命令显示到终端上，echo off 是取消命令的回显，@的作用是将echo off这行命令的回显也取消掉，%*是用来接受参数(jnb test.ipynb&#x3D;jupyter notebook test.ipynb)，所以运行时会执行”jupyter notebook %*”的命令，等同于在终端上输入了jupyter notebook。</p>
<hr>
<p>破案了，直接配vscode+jupyter更简单 </p>
]]></content>
      <tags>
        <tag>Jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>单调栈与单调队列</title>
    <url>//2025/02/12/monotone-stack-and-queue/</url>
    <content><![CDATA[<p>单调栈与单调队列一般存在以下三种形式：存值，存下标，数组实现，数组实现的优点在于在清空栈时只需top&#x3D;0即可</p>
<h1 id="单调栈"><a href="#单调栈" class="headerlink" title="单调栈"></a>单调栈</h1><p>伪代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for(i=1-&gt;n)//n为元素数量</span><br><span class="line">&#123;</span><br><span class="line">    ① while(栈非空且入栈元素优于栈顶元素)栈顶出栈//维持栈的单调性</span><br><span class="line">    //while结束的两种可能的判断</span><br><span class="line">    ② 如果(栈顶元素优于入栈元素)更新一次答案，此时栈顶即为最优解</span><br><span class="line">    否则 不存在答案</span><br><span class="line">    ③ 将新元素入栈</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>eg:输入n个数字 输出每个数字左侧距离该数字最近的比它小的元素，不存在则输出-1</strong><br>in      5<br>        7 8 5 6 7<br>out     -1 7 -1 5 6</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//存值</span></span><br><span class="line"><span class="type">int</span> a[N],ans[N];<span class="comment">//a[N]用来存放输入的数字，ans[N]用于存放答案</span></span><br><span class="line">stack&lt;<span class="type">int</span>&gt; stk;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)cin&gt;&gt;a[i];</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span>(stk.<span class="built_in">size</span>()&amp;&amp;stk.top&gt;=a[i])stk.<span class="built_in">pop</span>();</span><br><span class="line">    <span class="keyword">if</span>(stk.<span class="built_in">empty</span>())ans[i]=<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">else</span> ans[i]=stk.<span class="built_in">top</span>();</span><br><span class="line">    stk.<span class="built_in">push</span>(a[i]);</span><br><span class="line">    cout&lt;&lt;ans[i]&lt;&lt;<span class="string">&#x27; &#x27;</span>;</span><br><span class="line">&#125;   </span><br><span class="line"><span class="comment">//存下标＋数组实现</span></span><br><span class="line"><span class="type">int</span> a[N],stk[N],ans[N],top;<span class="comment">//stk数组用来模拟下标</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)cin&gt;&gt;a[i];</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span>(top&amp;&amp;a[stk[top]]&gt;=a[i])top--;</span><br><span class="line">    <span class="keyword">if</span>(top) ans[i]=a[stk[top]];</span><br><span class="line">    <span class="keyword">else</span> ans[i]=<span class="number">-1</span>;</span><br><span class="line">    stk[++top]=i;</span><br><span class="line">    cout&lt;&lt;ans[i]&lt;&lt;<span class="string">&#x27; &#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="单调队列"><a href="#单调队列" class="headerlink" title="单调队列"></a>单调队列</h1><p>通过deque实现，在队头进行出队操作，在队尾进行入队和出队操作，最典型的是用于处理滑动窗口<br>伪代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for(i=1-&gt;n)</span><br><span class="line">&#123;</span><br><span class="line">    ① while(非空且队头不合法)弹出头//队头合法性</span><br><span class="line">    while(非空且i优于队尾)弹出尾//队尾优越性</span><br><span class="line">    ② 将i入队</span><br><span class="line">    ③ 此时队头为最优</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="https://www.luogu.com.cn/problem/P1886"><em><strong>例题</strong></em></a></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//deque存下标实现</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//滑动窗口的区间为[i-k+1,i]</span></span><br><span class="line">    <span class="keyword">while</span> (dq.<span class="built_in">size</span>() &amp;&amp; dq.<span class="built_in">front</span>() &lt; i - k + <span class="number">1</span>)dq.<span class="built_in">pop_front</span>();</span><br><span class="line">    <span class="keyword">while</span> (dq.<span class="built_in">size</span>() &amp;&amp; a[i]&lt;= a[dq.<span class="built_in">back</span>()])dq.<span class="built_in">pop_back</span>();</span><br><span class="line">    dq.<span class="built_in">push_back</span>(i);</span><br><span class="line">    <span class="comment">//当第一个滑动窗口开始形成时，开始输出答案</span></span><br><span class="line">    <span class="keyword">if</span> (i &gt;= k)cout &lt;&lt; a[dq.<span class="built_in">front</span>()] &lt;&lt; <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>#TODO 数组模拟</p>
]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Jupyter7下的翻译问题的解决</title>
    <url>//2025/08/15/Jupyter7-translation-fix/</url>
    <content><![CDATA[<h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>jnb更新到7版本后，不少nbextension也随之失效，其中当然包括nbtranslate，</p>
<p>尽管jnb7是基于jlab的，有着jlab的扩展支持，但其中也没有翻译方面的扩展。</p>
<p>但不得不面对的是，不少ipynb是英文版本的，有些时候必然会造成困扰</p>
<h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p><del>提升英语水平，写一个python脚本，亦或者降级到jnb6()</del></p>
<p>大模型时代当然要使用大模型，可以直接将文件喂给大模型，但是不氪金会受到点限制<br>寻找solution时，逛到jnb的侧栏ai agent，感觉很是方便，而且模型能自己选择，缺点是不氪金一个月只能对话50次（<del>可以换着号来用</del>），但是只用来翻译够用了</p>
<p>下面呈现方法</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">pip install runcell</span><br></pre></td></tr></table></figure>

<p>在查看(View)-右侧边栏(Right Sidebar)-show打开，用github&#x2F;google登录即可</p>
<hr>
<p>破案了，直接vscode+jupyter+copilot完事</p>
]]></content>
      <tags>
        <tag>Jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMAFactory各参数详解</title>
    <url>//2026/01/30/LLaMAFactory-parameters-guide/</url>
    <content><![CDATA[<h3 id="1-模型与数据配置"><a href="#1-模型与数据配置" class="headerlink" title="1. 模型与数据配置"></a>1. 模型与数据配置</h3><table>
<thead>
<tr>
<th>参数&#x2F;选项</th>
<th>详细说明</th>
</tr>
</thead>
<tbody><tr>
<td>模型名称或路径</td>
<td>- 作用：指定要微调的基座模型。可以是 Hugging Face 上的模型标识符（如 <code>meta-llama/Llama-2-7b-chat-hf</code>），也可以是本地模型文件夹的路径。</td>
</tr>
<tr>
<td></td>
<td>- 注意：需要确保 LLaMA Factory 支持该模型架构，并且你有权限访问该模型。</td>
</tr>
<tr>
<td>适配器名称或路径</td>
<td>- 作用：如果之前进行过 LoRA 微调，可以在这里加载已有的 LoRA 适配器权重，用于继续训练或进行推理。</td>
</tr>
<tr>
<td>微调方法</td>
<td>- 作用：选择核心的微调策略。</td>
</tr>
<tr>
<td></td>
<td>- Full：全参数微调。消耗资源巨大，通常只在资源充足且需要最大程度改变模型时使用。</td>
</tr>
<tr>
<td></td>
<td>- Freeze：冻结微调。只训练模型的部分层（如最后几层），其余层参数冻结。是一种轻量级方法。</td>
</tr>
<tr>
<td></td>
<td>- LoRA：最常用。在原始模型旁增加低秩适配器，只训练这些小的适配器参数，极大减少显存和计算需求。</td>
</tr>
<tr>
<td></td>
<td>- QLoRA：LoRA 的量化版本。将基座模型以 4-bit 等量化方式加载，进一步降低显存需求，使得在消费级显卡上微调大模型成为可能。</td>
</tr>
<tr>
<td>数据集</td>
<td>- 作用：选择用于训练和评估的数据集。</td>
</tr>
<tr>
<td></td>
<td>- 格式：支持多种格式，如 <code>alpaca_gpt4_en</code>（指令微调常用格式）、自定义的 JSON 文件等。</td>
</tr>
<tr>
<td></td>
<td>- 组成：通常需要指定训练集和评估集。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>详细说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>stage</code></td>
<td>微调阶段，可选以下类型：</td>
</tr>
<tr>
<td></td>
<td><code>pt</code>（Pretraining）：预训练阶段。</td>
</tr>
<tr>
<td></td>
<td><code>sft</code>（Supervised Fine-tuning）：监督微调阶段。</td>
</tr>
<tr>
<td></td>
<td><code>rm</code>（Reward Modeling）：奖励建模阶段。</td>
</tr>
<tr>
<td></td>
<td><code>ppo</code>（Proximal Policy Optimization）：基于奖励的强化微调阶段。</td>
</tr>
<tr>
<td></td>
<td><code>dpo</code>（Direct Preference Optimization）：偏好优化阶段。</td>
</tr>
<tr>
<td></td>
<td><code>kto</code>（Knowledge Transfer Optimization）：知识迁移优化阶段。</td>
</tr>
<tr>
<td><code>do_train</code></td>
<td>是否执行训练。<code>true</code> 表示执行训练，<code>false</code> 表示跳过训练过程，仅用于推理或验证。</td>
</tr>
</tbody></table>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><table>
<thead>
<tr>
<th>参数名称</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td><code>dataset</code></td>
<td>在 <code>dataset_info.json</code> 中预设的数据集名称，以 <code>,</code> 分隔启用多个数据集。</td>
</tr>
<tr>
<td><code>template</code></td>
<td>数据处理模板。设置为 <code>llama3</code>，表明按照适配 Llama 3 模型的格式预处理数据。</td>
</tr>
<tr>
<td><code>cutoff_len</code></td>
<td>输入序列的最大长度。2048 表示限制每条样本的最大 token 数。</td>
</tr>
<tr>
<td><code>max_samples</code></td>
<td>数据集中的最大样本数量，设置为 1000，用于减少训练规模或调试。</td>
</tr>
<tr>
<td><code>overwrite_cache</code></td>
<td>是否覆盖缓存的预处理数据，<code>true</code> 表示重新处理。</td>
</tr>
<tr>
<td><code>preprocessing_num_workers</code></td>
<td>数据预处理的并行线程数，设置为 16，提高处理效率。</td>
</tr>
</tbody></table>
<h3 id="2-训练配置"><a href="#2-训练配置" class="headerlink" title="2. 训练配置"></a>2. 训练配置</h3><table>
<thead>
<tr>
<th>参数</th>
<th>详细说明</th>
</tr>
</thead>
<tbody><tr>
<td>学习率</td>
<td>- 作用：控制模型参数更新的步长。是最重要的超参数之一。</td>
</tr>
<tr>
<td></td>
<td>- 值域：通常是一个很小的值，如 <code>1e-4</code> 到 <code>5e-5</code>。太大可能导致训练不稳定（损失爆炸），太小则收敛缓慢。</td>
</tr>
<tr>
<td>训练轮数</td>
<td>- 作用：整个训练数据集会被模型“看过”多少次。</td>
</tr>
<tr>
<td></td>
<td>- 选择：并非越多越好。太多会导致过拟合，即模型在训练集上表现很好，但在新数据上表现很差。需要配合评估损失来观察。</td>
</tr>
<tr>
<td>最大样本数</td>
<td>- 作用：限制每个训练轮次中使用的最大样本数量。用于快速测试或在小规模数据上实验。</td>
</tr>
<tr>
<td>学习率调度器</td>
<td>- 作用：动态调整学习率的策略。</td>
</tr>
<tr>
<td></td>
<td>- Linear：线性衰减，学习率从初始值线性下降到 0。</td>
</tr>
<tr>
<td></td>
<td>- Cosine：余弦衰减，学习率按余弦曲线平滑下降，是常用且效果较好的选择。</td>
</tr>
<tr>
<td></td>
<td>- Constant：保持学习率不变。</td>
</tr>
<tr>
<td>最大梯度范数</td>
<td>- 作用：梯度裁剪的阈值。当梯度的范数超过此值时，会将其缩放至此范数。</td>
</tr>
<tr>
<td></td>
<td>- 目的：防止梯度爆炸，稳定训练过程。通常设置为 <code>1.0</code> 或 <code>0.5</code>。</td>
</tr>
<tr>
<td>验证步数</td>
<td>- 作用：每训练多少步，就在验证集上评估一次并计算损失。</td>
</tr>
<tr>
<td>保存步数</td>
<td>- 作用：每训练多少步，就保存一次模型检查点（通常是 LoRA 适配器）。</td>
</tr>
<tr>
<td>优化器</td>
<td>- 作用：选择用于更新模型参数的算法。</td>
</tr>
<tr>
<td></td>
<td>- AdamW：最常用的优化器，是 Adam 的一个变种，能更好地处理权重衰减。</td>
</tr>
<tr>
<td></td>
<td>- AdamW 8bit：AdamW 的 8 位量化版本，可以节省一些显存。</td>
</tr>
<tr>
<td></td>
<td>- SGD：随机梯度下降，更古老但稳定的算法。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>典型值</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td><code>per_device_train_batch_size</code></td>
<td>1</td>
<td>每张 GPU 上的训练批量大小，影响梯度的估计质量。较大的批量能提供更稳定的梯度，适合较大的学习率；较小的批量可能导致噪声较大。小批量大小（例如 1）需要配合<code>gradient_accumulation_steps</code>才能等效于更大的批量。</td>
</tr>
<tr>
<td><code>gradient_accumulation_steps</code></td>
<td>8</td>
<td>梯度累积步数，累积梯度可等效增大批量大小，影响模型的收敛性和泛化性能。</td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td>2e-4</td>
<td>初始学习率，最关键的超参数之一，直接影响训练的稳定性和收敛性。</td>
</tr>
<tr>
<td><code>num_train_epochs</code></td>
<td>3.0</td>
<td>训练总轮数，微调过程中，模型通常已经有良好的初始化，因此较少的轮数往往就足够。</td>
</tr>
<tr>
<td><code>lr_scheduler_type</code></td>
<td>cosine</td>
<td>学习率调度策略，决定了学习率如何变化；<code>cosine</code> 表示采用余弦退火策略，能够有效降低后期的学习率，通常对大模型训练表现较好。</td>
</tr>
<tr>
<td><code>warmup_ratio</code></td>
<td>0.1</td>
<td>学习率预热阶段的比例，预热阶段可以防止训练一开始因为学习率过高而导致的梯度爆炸；<code>0.1</code>表示前 10%的训练步数用于学习率预热。</td>
</tr>
<tr>
<td><code>bf16</code></td>
<td>true</td>
<td>是否启用 bfloat16 混合精度训练，启用可减小内存占用并加速训练。</td>
</tr>
<tr>
<td><code>ddp_timeout</code></td>
<td>180000000</td>
<td>分布式数据并行的超时时间，设置为非常大的值，确保分布式训练不会因为超时失败。</td>
</tr>
</tbody></table>
<h3 id="3-LoRA-QLoRA-配置"><a href="#3-LoRA-QLoRA-配置" class="headerlink" title="3. LoRA&#x2F;QLoRA 配置"></a>3. LoRA&#x2F;QLoRA 配置</h3><p>当微调方法选择 LoRA 或 QLoRA 时，这些参数变得至关重要。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>详细说明</th>
</tr>
</thead>
<tbody><tr>
<td>LoRA Rank</td>
<td>- 作用：LoRA 适配器中<strong>矩阵的秩（<code>r</code>）</strong>。它决定了适配器的复杂度和参数量。</td>
</tr>
<tr>
<td></td>
<td>- 值域：通常是 <code>8, 16, 32, 64</code>。值越大，能力越强，但参数量和计算量也越大。对于大多数任务，<code>8</code> 或 <code>16</code> 已经足够。</td>
</tr>
<tr>
<td>LoRA Alpha</td>
<td>- 作用：LoRA 适配器输出结果的<strong>缩放因子</strong>。可以理解为适配器对原始模型影响力的控制参数。</td>
</tr>
<tr>
<td></td>
<td>- 经验法则：通常设置为 LoRA Rank 的 1 到 2 倍（如 <code>rank=8, alpha=16</code>）。这被认为是一个好的起点。</td>
</tr>
<tr>
<td>Dropout</td>
<td>- 作用：在 LoRA 适配器中应用 Dropout 的比例，是一种防止过拟合的正则化手段。</td>
</tr>
<tr>
<td></td>
<td>- 值域：<code>0</code> 到 <code>1</code>。在小数据集上可以设置一个较小的值（如 <code>0.1</code>），在大数据集上可以设为 <code>0</code>。</td>
</tr>
<tr>
<td>LoRA 目标模块</td>
<td>- 作用：指定在<strong>模型的哪些部分</strong>添加 LoRA 适配器。这是 LoRA 微调最关键的配置之一。</td>
</tr>
<tr>
<td></td>
<td>- 常见值：对于 Llama 类模型，通常是 <code>q_proj, v_proj, k_proj, o_proj</code>（注意力机制的核心投影层）。有时也会加上 <code>gate_proj, up_proj, down_proj</code>（FFN 层）。</td>
</tr>
<tr>
<td></td>
<td>- All：如果选择 All，框架会自动为所有符合条件的线性层添加 LoRA。</td>
</tr>
<tr>
<td>量化比特数</td>
<td>- 作用：QLoRA 特有。指定基座模型的量化精度。</td>
</tr>
<tr>
<td></td>
<td>- 选项：<code>4-bit</code> 是最常用的，在性能和显存节省之间取得了很好的平衡。<code>8-bit</code> 也是一个选项。</td>
</tr>
</tbody></table>
<h3 id="4-生成-推理配置"><a href="#4-生成-推理配置" class="headerlink" title="4. 生成&#x2F;推理配置"></a>4. 生成&#x2F;推理配置</h3><table>
<thead>
<tr>
<th>参数</th>
<th>详细说明</th>
</tr>
</thead>
<tbody><tr>
<td>Do Sample</td>
<td>- 作用：是否使用采样策略。如果为 <code>False</code>，则使用贪心解码（总是选择概率最大的下一个词），生成结果确定但可能枯燥。</td>
</tr>
<tr>
<td>Temperature</td>
<td>- 作用：调整采样随机性的“温度”。</td>
</tr>
<tr>
<td></td>
<td>- 值域：<code>0</code> 到 <code>1</code> 或更高。</td>
</tr>
<tr>
<td></td>
<td>- <code>~0</code>：输出更确定，倾向于高概率词。</td>
</tr>
<tr>
<td></td>
<td>- <code>~1</code>：平衡的随机性。</td>
</tr>
<tr>
<td></td>
<td>- <code>&gt;1</code>：输出更随机，更具创造性，但也可能不连贯。</td>
</tr>
<tr>
<td>Top-p</td>
<td>- 作用：核采样。从累积概率超过 <code>p</code> 的最小词集合中采样。</td>
</tr>
<tr>
<td></td>
<td>- 效果：能动态控制候选词的范围，避免选择那些概率极低的词，同时保持多样性。通常与 <code>Temperature</code> 一起使用。常用值为 <code>0.9</code>。</td>
</tr>
<tr>
<td>Top-k</td>
<td>- 作用：从概率最高的 <code>k</code> 个词中进行采样。</td>
</tr>
<tr>
<td></td>
<td>- 效果：另一种限制采样范围的方法。<code>Top-p</code> 通常比 <code>Top-k</code> 更灵活和有效。</td>
</tr>
<tr>
<td>Max New Tokens</td>
<td>- 作用：生成文本的最大长度（Token 数量）。</td>
</tr>
<tr>
<td>Repetition Penalty</td>
<td>- 作用：对已生成过的词进行惩罚，降低其再次被生成的概率，有效减少重复。</td>
</tr>
<tr>
<td></td>
<td>- 值域：通常 <code>1.0</code> 表示无惩罚，<code>&gt;1.0</code>（如 <code>1.1</code> 到 <code>1.2</code>）表示施加惩罚。</td>
</tr>
</tbody></table>
<h3 id="数据集格式"><a href="#数据集格式" class="headerlink" title="数据集格式"></a><strong>数据集格式</strong></h3><p><strong>llama-factory支持的数据集格式：</strong></p>
<p>数据集通常是一个 JSON 文件（如 <code>.json</code>、<code>.jsonl</code>），每条数据包含一个对话或指令样本。<br>支持以下字段：</p>
<ul>
<li><code>instruction</code>：指令描述（可选，如果使用 <code>input</code> 和 <code>output</code>）</li>
<li><code>input</code>：输入内容（可选）</li>
<li><code>output</code>：期望输出</li>
<li><code>messages</code>：对话列表（用于多轮对话）</li>
<li><code>system</code>：系统提示（可选）</li>
<li><code>history</code>：历史对话（可选，用于多轮）</li>
</ul>
<h2 id="监督训练"><a href="#监督训练" class="headerlink" title="监督训练"></a><strong>监督训练</strong></h2><h2 id="单轮指令格式"><a href="#单轮指令格式" class="headerlink" title="单轮指令格式"></a>单轮指令格式</h2><h3 id="格式1："><a href="#格式1：" class="headerlink" title="格式1："></a>格式1：</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;翻译成英文&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今天天气真好&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The weather is nice today.&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>instruction</code>：任务指令</li>
<li><code>input</code>：任务输入（可为空）</li>
<li><code>output</code>：期望输出</li>
</ul>
<h3 id="格式2：（ChatML-格式）"><a href="#格式2：（ChatML-格式）" class="headerlink" title="格式2：（ChatML 格式）"></a>格式2：（ChatML 格式）</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;messages&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今天天气真好，请翻译成英文。&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The weather is nice today.&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>必须包含 <code>user</code> 和 <code>assistant</code> 交替的对话。</li>
<li>可包含 <code>system</code> 消息。</li>
</ul>
<h2 id="多轮对话格式"><a href="#多轮对话格式" class="headerlink" title="多轮对话格式"></a>多轮对话格式</h2><h3 id="格式1：-1"><a href="#格式1：-1" class="headerlink" title="格式1："></a>格式1：</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;messages&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;system&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你是一个翻译助手。&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;翻译：今天天气真好&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The weather is nice today.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;改成过去时&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The weather was nice today.&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>严格按 <code>user</code> → <code>assistant</code> → <code>user</code> → <code>assistant</code> 顺序。</p>
<h3 id="格式2："><a href="#格式2：" class="headerlink" title="格式2："></a>格式2：</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;history&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">[</span><span class="string">&quot;今天天气真好&quot;</span><span class="punctuation">,</span> <span class="string">&quot;The weather is nice today.&quot;</span><span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;改成过去时&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The weather was nice today.&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>history</code>：列表形式，每轮是 <code>[用户输入, 助手回复]</code>。</li>
<li><code>input</code>：当前轮用户输入。</li>
<li><code>output</code>：当前轮助手回复。</li>
</ul>
<p>数据集目录通常包含：</p>
<ul>
<li><ul>
<li><code>dataset.json</code>（或 <code>.jsonl</code>）</li>
<li>可拆分：<code>train.json</code>、<code>validation.json</code>、<code>test.json</code></li>
</ul>
</li>
</ul>
<h2 id="无监督训练"><a href="#无监督训练" class="headerlink" title="无监督训练"></a>无监督训练</h2><h3 id="格式1：JSONL"><a href="#格式1：JSONL" class="headerlink" title="格式1：JSONL"></a>格式1：JSONL</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&quot;text&quot;: &quot;这是第一个文档的内容，可以是任意长度的纯文本。\n可以包含换行符和标点。&quot;&#125;</span><br><span class="line">&#123;&quot;text&quot;: &quot;这是第二个文档。无监督训练只需要纯文本，不需要指令或回复。&quot;&#125;</span><br><span class="line">&#123;&quot;text&quot;: &quot;中文、英文或混合文本都可以。模型会根据上下文学习语言模式。&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>每个 JSON 对象包含一个 <code>text</code> 字段</p>
<h3 id="参数含义"><a href="#参数含义" class="headerlink" title="参数含义"></a>参数含义</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&#x27;loss&#x27;: 1.2992, ...&#125;  # 当前批次的训练损失</span><br><span class="line">&#123;&#x27;train_loss&#x27;: 0.0, ...&#125;  # 最终平均训练损失（这里有问题，正常不应为0）</span><br><span class="line"></span><br><span class="line"># 评估过程</span><br><span class="line">&#123;&#x27;eval_loss&#x27;: 1.275&#125;  # 验证集上的损失</span><br><span class="line">&#123;&#x27;learning_rate&#x27;: 1.0732e-05, ...&#125;  # 当前学习率</span><br><span class="line">&#123;&#x27;epoch&#x27;: 2.10, ...&#125;  # 当前训练轮数（含小数表示进度）</span><br><span class="line"></span><br><span class="line">&#123;&#x27;throughput&#x27;: 1226.07&#125;  # 吞吐量（tokens/秒）</span><br><span class="line">&#123;&#x27;train_samples_per_second&#x27;: 80145.936&#125;  # 样本处理速度</span><br><span class="line">&#123;&#x27;train_steps_per_second&#x27;: 5120.435&#125;    # 训练步数速度</span><br><span class="line"></span><br><span class="line">&#123;&#x27;trainable params: 5,046,272&#x27;&#125;      # 可训练参数（LoRA参数）</span><br><span class="line">&#123;&#x27;all params: 601,096,192&#x27;&#125;          # 总参数（原始模型+LoRA）</span><br><span class="line">&#123;&#x27;trainable%: 0.8395&#x27;&#125;               # 可训练参数占比</span><br><span class="line"></span><br><span class="line">&#123;&#x27;num_input_tokens_seen&#x27;: 700704&#125;    # 处理的总token数</span><br><span class="line">&#123;&#x27;total_flos&#x27;: 1744403GF&#x27;&#125;          # 总浮点运算次数（1.74×10^18）</span><br><span class="line"></span><br><span class="line">&#x27;Upcasting trainable params to float32&#x27;  # LoRA参数用FP32训练</span><br><span class="line">&#x27;Fine-tuning method: LoRA&#x27;  # 使用LoRA方法</span><br><span class="line">&#x27;Loaded adapter(s): saves/Qwen3-0.6B-Base/lora/...&#x27;  # 加载已有LoRA权重</span><br><span class="line">&#x27;per_device_train_batch_size&#x27;: 4 # 每个GPU的物理批次大小 </span><br><span class="line">&#x27;gradient_accumulation_steps&#x27;: 8 # 梯度累积步数</span><br><span class="line">实际有效批次大小 = per_device_train_batch_size × gradient_accumulation_steps × GPU数量</span><br></pre></td></tr></table></figure>

<h3 id="yaml文件配置"><a href="#yaml文件配置" class="headerlink" title="yaml文件配置"></a>yaml文件配置</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基础模型配置</span></span><br><span class="line"><span class="attr">model_name_or_path:</span> <span class="string">Qwen/Qwen2.5-VL-7B-Instruct</span></span><br><span class="line"><span class="comment"># 作用：指定预训练模型名称或本地路径</span></span><br><span class="line"><span class="comment"># 支持：HuggingFace模型ID或本地目录</span></span><br><span class="line"></span><br><span class="line"><span class="attr">image_max_pixels:</span> <span class="number">262144</span></span><br><span class="line"><span class="comment"># 作用：图像最大像素数（512×512）</span></span><br><span class="line"><span class="comment"># 计算：512×512=262144</span></span><br><span class="line"><span class="comment"># 影响：图像分辨率，减少显存占用</span></span><br><span class="line"></span><br><span class="line"><span class="attr">video_max_pixels:</span> <span class="number">16384</span></span><br><span class="line"><span class="comment"># 作用：视频最大像素数（128×128）</span></span><br><span class="line"><span class="comment"># 计算：128×128=16384</span></span><br><span class="line"><span class="comment"># 注意：视频通常需要更多显存</span></span><br><span class="line"></span><br><span class="line"><span class="attr">trust_remote_code:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 作用：信任远程代码执行</span></span><br><span class="line"><span class="comment"># 必要性：某些模型需要自定义代码加载</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#LORA配置</span></span><br><span class="line"><span class="attr">stage:</span> <span class="string">sft</span></span><br><span class="line"><span class="comment"># 作用：训练阶段</span></span><br><span class="line"><span class="comment"># 选项：pretrain（预训练）、sft（监督微调）、rm（奖励模型）、ppo（强化学习）</span></span><br><span class="line"></span><br><span class="line"><span class="attr">do_train:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 作用：是否执行训练</span></span><br><span class="line"></span><br><span class="line"><span class="attr">finetuning_type:</span> <span class="string">lora</span></span><br><span class="line"><span class="comment"># 作用：微调类型</span></span><br><span class="line"><span class="comment"># 选项：full（全参数）、lora（LoRA）、freeze（冻结）、none（仅推理）</span></span><br><span class="line"></span><br><span class="line"><span class="attr">lora_rank:</span> <span class="number">8</span></span><br><span class="line"><span class="comment"># 作用：LoRA秩（低秩矩阵的维度）</span></span><br><span class="line"><span class="comment"># 范围：通常4-64，越小参数量越少，能力可能下降</span></span><br><span class="line"></span><br><span class="line"><span class="attr">lora_target:</span> <span class="string">all</span></span><br><span class="line"><span class="comment"># 作用：LoRA适配的模块</span></span><br><span class="line"><span class="comment"># 选项：all（所有线性层）、q_proj,v_proj（仅注意力层）、自定义</span></span><br><span class="line"><span class="comment"># 影响：all参数量最多，但效果可能最好</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集配置</span></span><br><span class="line"><span class="attr">dataset:</span> <span class="string">mllm_demo,identity,alpaca_en_demo</span></span><br><span class="line"><span class="comment"># 作用：使用的数据集名称</span></span><br><span class="line"><span class="comment"># 格式：dataset_info.json中定义的key，逗号分隔</span></span><br><span class="line"></span><br><span class="line"><span class="attr">template:</span> <span class="string">qwen2_vl</span></span><br><span class="line"><span class="comment"># 作用：对话模板</span></span><br><span class="line"><span class="comment"># 必要性：不同模型需要不同对话格式</span></span><br><span class="line"><span class="comment"># 选项：llama3、qwen、chatml等</span></span><br><span class="line"></span><br><span class="line"><span class="attr">cutoff_len:</span> <span class="number">2048</span></span><br><span class="line"><span class="comment"># 作用：序列最大长度</span></span><br><span class="line"><span class="comment"># 影响：超过部分截断，影响长文本处理</span></span><br><span class="line"></span><br><span class="line"><span class="attr">max_samples:</span> <span class="number">1000</span></span><br><span class="line"><span class="comment"># 作用：每个数据集最大样本数</span></span><br><span class="line"><span class="comment"># 用途：快速测试或限制数据量</span></span><br><span class="line"></span><br><span class="line"><span class="attr">overwrite_cache:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 作用：覆盖已存在的缓存</span></span><br><span class="line"><span class="comment"># 用途：数据更新后重新预处理</span></span><br><span class="line"></span><br><span class="line"><span class="attr">preprocessing_num_workers:</span> <span class="number">16</span></span><br><span class="line"><span class="comment"># 作用：数据预处理的进程数</span></span><br><span class="line"><span class="comment"># 优化：CPU密集型任务，通常设CPU核心数</span></span><br><span class="line"></span><br><span class="line"><span class="attr">dataloader_num_workers:</span> <span class="number">4</span></span><br><span class="line"><span class="comment"># 作用：数据加载的进程数</span></span><br><span class="line"><span class="comment"># 优化：IO密集型任务，通常设4-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#输出配置</span></span><br><span class="line"><span class="attr">output_dir:</span> <span class="string">saves/qwen2_5vl-7b/lora/sft</span></span><br><span class="line"><span class="comment"># 作用：输出目录</span></span><br><span class="line"><span class="comment"># 结构：包含模型权重、日志、配置</span></span><br><span class="line"></span><br><span class="line"><span class="attr">logging_steps:</span> <span class="number">10</span></span><br><span class="line"><span class="comment"># 作用：每多少训练步记录一次日志</span></span><br><span class="line"><span class="comment"># 监控：损失、学习率等指标</span></span><br><span class="line"></span><br><span class="line"><span class="attr">save_steps:</span> <span class="number">500</span></span><br><span class="line"><span class="comment"># 作用：每多少步保存一次检查点</span></span><br><span class="line"><span class="comment"># 容灾：训练中断后可恢复</span></span><br><span class="line"></span><br><span class="line"><span class="attr">plot_loss:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 作用：绘制损失曲线图</span></span><br><span class="line"><span class="comment"># 输出：training_loss.png</span></span><br><span class="line"></span><br><span class="line"><span class="attr">overwrite_output_dir:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 作用：覆盖已有输出目录</span></span><br><span class="line"><span class="comment"># 警告：会删除之前的内容</span></span><br><span class="line"></span><br><span class="line"><span class="attr">save_only_model:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># 作用：是否只保存模型权重</span></span><br><span class="line"><span class="comment"># true：只保存pytorch_model.bin</span></span><br><span class="line"><span class="comment"># false：保存完整检查点（含优化器状态）</span></span><br><span class="line"></span><br><span class="line"><span class="attr">report_to:</span> <span class="string">none</span></span><br><span class="line"><span class="comment"># 作用：实验跟踪工具</span></span><br><span class="line"><span class="comment"># 选项：wandb、tensorboard、mlflow等</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练参数配置</span></span><br><span class="line"><span class="attr">per_device_train_batch_size:</span> <span class="number">1</span></span><br><span class="line"><span class="comment"># 作用：每个GPU的物理批次大小</span></span><br><span class="line"><span class="comment"># 显存：决定单次前向传播的样本数</span></span><br><span class="line"></span><br><span class="line"><span class="attr">gradient_accumulation_steps:</span> <span class="number">8</span></span><br><span class="line"><span class="comment"># 作用：梯度累积步数</span></span><br><span class="line"><span class="comment"># 计算：有效批次 = 1 × 8 = 8</span></span><br><span class="line"><span class="comment"># 显存：用8倍时间换8倍显存节省</span></span><br><span class="line"></span><br><span class="line"><span class="attr">learning_rate:</span> <span class="number">1.0e-4</span></span><br><span class="line"><span class="comment"># 作用：学习率</span></span><br><span class="line"><span class="comment"># 范围：LoRA通常1e-4到5e-4，全参数微调更小</span></span><br><span class="line"></span><br><span class="line"><span class="attr">num_train_epochs:</span> <span class="number">3.0</span></span><br><span class="line"><span class="comment"># 作用：训练轮数</span></span><br><span class="line"><span class="comment"># 经验：SFT通常3-10轮</span></span><br><span class="line"></span><br><span class="line"><span class="attr">lr_scheduler_type:</span> <span class="string">cosine</span></span><br><span class="line"><span class="comment"># 作用：学习率调度器</span></span><br><span class="line"><span class="comment"># 选项：linear、cosine、cosine_with_restarts、constant</span></span><br><span class="line"><span class="comment"># cosine：余弦衰减，平滑收敛</span></span><br><span class="line"></span><br><span class="line"><span class="attr">warmup_ratio:</span> <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 作用：学习率预热比例</span></span><br><span class="line"><span class="comment"># 计算：前10%的步数从0线性增加到目标学习率</span></span><br><span class="line"><span class="comment"># 目的：稳定训练初期</span></span><br><span class="line"></span><br><span class="line"><span class="attr">bf16:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 作用：使用bfloat16混合精度</span></span><br><span class="line"><span class="comment"># 要求：Ampere架构以上GPU（A100/3090/4090等）</span></span><br><span class="line"><span class="comment"># 替代：fp16（V100等旧卡）</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ddp_timeout:</span> <span class="number">180000000</span></span><br><span class="line"><span class="comment"># 作用：分布式训练超时时间（秒）</span></span><br><span class="line"><span class="comment"># 默认：1800秒，这里设很大（约5.7年）</span></span><br><span class="line"><span class="comment"># 用途：禁用超时，用于长时间训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估参数配置</span></span><br><span class="line"><span class="attr">val_size:</span> <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 作用：验证集比例</span></span><br><span class="line"><span class="comment"># 计算：从训练数据中划分10%作为验证集</span></span><br><span class="line"></span><br><span class="line"><span class="attr">per_device_eval_batch_size:</span> <span class="number">1</span></span><br><span class="line"><span class="comment"># 作用：评估时的批次大小</span></span><br><span class="line"><span class="comment"># 注意：通常比训练批次大，因为不计算梯度</span></span><br><span class="line"></span><br><span class="line"><span class="attr">eval_strategy:</span> <span class="string">steps</span></span><br><span class="line"><span class="comment"># 作用：评估策略</span></span><br><span class="line"><span class="comment"># 选项：steps（按步数）、epoch（按轮数）、no（不评估）</span></span><br><span class="line"></span><br><span class="line"><span class="attr">eval_steps:</span> <span class="number">500</span></span><br><span class="line"><span class="comment"># 作用：每多少训练步评估一次</span></span><br></pre></td></tr></table></figure>

<h3 id="数据集template模板"><a href="#数据集template模板" class="headerlink" title="数据集template模板"></a>数据集template模板</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">&#123;</span><br><span class="line">    &quot;instruction&quot;: &quot;解释人工智能&quot;,</span><br><span class="line">    &quot;input&quot;: &quot;&quot;,</span><br><span class="line">    &quot;output&quot;: &quot;人工智能是...&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 应用 template 后</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">解释人工智能&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">人工智能是...&lt;|im_end|&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<p>不同模型所需的模板：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Llama 3 模板</span><br><span class="line">&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span><br><span class="line">&#123;instruction&#125;&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span><br><span class="line">&#123;output&#125;&lt;|eot_id|&gt;</span><br><span class="line"></span><br><span class="line"># Qwen 模板</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">&#123;instruction&#125;&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&#123;output&#125;&lt;|im_end|&gt;</span><br><span class="line"></span><br><span class="line"># ChatGLM 模板</span><br><span class="line">[gMASK]sop&lt;|user|&gt;</span><br><span class="line">&#123;instruction&#125;</span><br><span class="line">&lt;|assistant|&gt;</span><br><span class="line">&#123;output&#125;</span><br><span class="line"></span><br><span class="line"># 添加特殊分隔符</span><br><span class="line">bos_token: &quot;&lt;s&gt;&quot;          # 开始符</span><br><span class="line">eos_token: &quot;&lt;/s&gt;&quot;         # 结束符</span><br><span class="line">pad_token: &quot;[PAD]&quot;        # 填充符</span><br><span class="line"></span><br><span class="line"># 角色标记</span><br><span class="line">user_token: &quot;Human:&quot;</span><br><span class="line">assistant_token: &quot;Assistant:&quot;</span><br><span class="line">system_token: &quot;System:&quot;</span><br></pre></td></tr></table></figure>

<p>正确模板：模型能识别对话结构<br>错误模板：模型学习混乱的格式，难以收敛<br>训练时用的模板必须与推理时一致,否则生成的内容格式错误</p>
]]></content>
      <tags>
        <tag>LLM</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>深度神经网络DNN部分原理</title>
    <url>//2026/02/12/DNN/</url>
    <content><![CDATA[<h1 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h1><p><img src="/img/shenjing.png" alt="神经网络"></p>
<p>上图是一个四层的神经网络，有三个隐藏层。</p>
<p>用L表示层数，$n^{[l]}$表示第l个隐藏层的神经元数目，$a^{[l]}$来记作l层激活后的结果。用激活函数$g^{[l]}$计算$z^{[l]}$</p>
<h2 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h2><p><strong>前向传播</strong>的步骤可以写成<br>$$<br>z^{[l]}&#x3D;W^{[l]}·A^{[l-1]}+b^{[l]}\tag{1}<br>$$</p>
<p>$$<br>A^{[l]}&#x3D;g^{[l]}(Z^{[l]})\tag{2}<br>$$</p>
<p>前向传播需要喂入$A^{[0]}$也就是$X$，来初始化；初始化的是第一层的输入值。$a^{[0]}$对应于一个训练样本的输入特征，而$A^{[0]}$对应于一整个训练样本的输入特征，所以这就是这条链的第一个前向函数的输入，重复这个步骤就可以从左到右计算前向传播。</p>
<p><strong>反向传播</strong>的步骤可以写成<br>$$<br>dz^{[l]}&#x3D;da^{[l]}*g’^{[l]}(z^{[l]})\tag{3}<br>$$</p>
<p>$$<br>dw^{[l]}&#x3D;dz^{[l]}·a^{[l-1]}\tag{4}<br>$$</p>
<p>$$<br>db^{[l]}&#x3D;dz^{[l]}\tag{5}<br>$$</p>
<p>$$<br>da^{[l-1]}&#x3D;dz^{[l]}·W^{[l]T}\tag{6}<br>$$</p>
<p><strong>向量化表示</strong><br>$$<br>dZ^{[l]} &#x3D; dA^{[l]} * g’^{[l]}(Z^{[l]})\tag{7}<br>$$</p>
<p>$$<br>dW^{[l]} &#x3D; \frac{1}{m} dZ^{[l]} \cdot A^{[l-1]T}\tag{8}<br>$$</p>
<p>$$<br>db^{[l]} &#x3D; \frac{1}{m} np.sum(dZ^{[l]}, axis &#x3D; 1, keepdims &#x3D; True)\tag{9}<br>$$</p>
<p>$$<br>dA^{[l-1]} &#x3D; W^{[l]T} \cdot dZ^{[l]}\tag{10}<br>$$</p>
<p>正向传播是来预测的，反向传播是用来求梯度的，求出来的梯度会用梯度下降法牛顿法等优化方法求解权重w和b，进而用公式$w^{[l]}&#x3D;w^{[l]}-\alpha dw^{[l]}$,$b^{[l]}&#x3D;b^{[l]}-\alpha db^{[l]}$更新参数</p>
<h2 id="核对矩阵的维数"><a href="#核对矩阵的维数" class="headerlink" title="核对矩阵的维数"></a>核对矩阵的维数</h2><p>实现深度神经网络时，可以拿出一张纸过一遍算法中矩阵的维数来当作检查代码是否有错的方法。</p>
<p>$w$的维度是（下一层的维数，上一层的维数），即$w^{[l]}:(n^{[l]},n^{[l-1]})$</p>
<p>$b$的维度是（下一层的维数，1），即$b^{[l]}:(n^{[l]},1)$</p>
<p>$z^{[l]},a^{[l]}:(n^{[l]},1)$</p>
<p>$dw^{[l]}$和$w^{[l]}$维度相同，$db^{[l]}$和$b^{[l]}$维度相同，且$w$和$b$向量化维度不变，但$z,a$以及$x$的维度向量化后会发生变化</p>
<p><strong>向量化后:</strong></p>
<p>$Z^{[l]}$可以看成由每个单独的$Z^{[l]}$叠加而得到，$Z^{[l]}&#x3D;(z^{[l][1]},z^{[l][2]},z^{[l][3]},…,z^{[l][m]})$</p>
<p>m为训练集大小，所以其维度不再是$(n^{[l]},1)$，而是$(n^{[l]},m)$</p>
<p>$A^{[l]}:(n^{[l]},m)$</p>
<p>在做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大大提高代码通过率。</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>总体思想就是由简单到复杂，先去考虑单层&#x2F;两层神经网络的情况，再将其作为辅助函数进行扩展</p>
<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><h3 id="两层神经网路"><a href="#两层神经网路" class="headerlink" title="两层神经网路"></a>两层神经网路</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    n_x -- 输入层大小</span></span><br><span class="line"><span class="string">    n_h -- 隐藏层大小</span></span><br><span class="line"><span class="string">    n_y -- 输出层大小</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    parameters -- 一个 Python 字典，包含你的参数：</span></span><br><span class="line"><span class="string">                    W1 -- 形状为 (n_h, n_x) 的权重矩阵</span></span><br><span class="line"><span class="string">                    b1 -- 形状为 (n_h, 1) 的偏置向量</span></span><br><span class="line"><span class="string">                    W2 -- 形状为 (n_y, n_h) 的权重矩阵</span></span><br><span class="line"><span class="string">                    b2 -- 形状为 (n_y, 1) 的偏置向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    //即时报错，便于后期调试</span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<h3 id="L层神经网络"><a href="#L层神经网络" class="headerlink" title="L层神经网络"></a>L层神经网络</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters_deep</span>(<span class="params">layer_dims</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    layer_dims -- Python 数组（list），包含网络每一层的维度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    parameters -- 一个 Python 字典，包含你的参数 &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;：</span></span><br><span class="line"><span class="string">                    Wl -- 形状为 (layer_dims[l], layer_dims[l-1]) 的权重矩阵</span></span><br><span class="line"><span class="string">                    bl -- 形状为 (layer_dims[l], 1) 的偏置向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layer_dims)            <span class="comment"># 网络中的层数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span>+<span class="built_in">str</span>(i)] = np.random.randn(layer_dims[i],layer_dims[i-<span class="number">1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span>+<span class="built_in">str</span>(i)] = np.zeros((layer_dims[i],<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(i)].shape == (layer_dims[i], layer_dims[i-<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(i)].shape == (layer_dims[i], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>- LINEAR</p>
<p>- LINEAR -&gt; ACTIVATION，其中 ACTIVATION 可以是 ReLU 或 Sigmoid。</p>
<p>- [LINEAR -&gt; RELU] $\times$ (L-1) -&gt; LINEAR -&gt; SIGMOID（整个模型）</p>
<h3 id="线性前向传播"><a href="#线性前向传播" class="headerlink" title="线性前向传播"></a>线性前向传播</h3><p>线性前向模块（对所有样本进行向量化）计算如下公式：<br>$$<br>Z^{[l]} &#x3D; W^{[l]}A^{[l-1]} +b^{[l]}\tag{1}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_forward</span>(<span class="params">A, W, b</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现某一层前向传播的线性部分。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    A -- 来自前一层的激活值（或输入数据）：(前一层大小, 样本数)</span></span><br><span class="line"><span class="string">    W -- 权重矩阵：形状为 (当前层大小, 前一层大小) 的 numpy 数组</span></span><br><span class="line"><span class="string">    b -- 偏置向量：形状为 (当前层大小, 1) 的 numpy 数组</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    Z -- 激活函数的输入（也称为 pre-activation 参数）</span></span><br><span class="line"><span class="string">    cache -- 一个 Python 元组，包含 &quot;A&quot;、&quot;W&quot; 和 &quot;b&quot;；用于高效计算反向传播</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    Z = np.dot(W,A)+b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure>

<h3 id="线性-激活前向传播"><a href="#线性-激活前向传播" class="headerlink" title="线性-激活前向传播"></a>线性-激活前向传播</h3><p>两种激活函数的引入：</p>
<ul>
<li><strong>Sigmoid</strong>：$\sigma(Z) &#x3D; \sigma(W A + b) &#x3D; \frac{1}{ 1 + e^{-(W A + b)}}$。已实现 <code>sigmoid</code> 函数（见附录）。该函数会返回 <strong>两个</strong> 值：激活值 “<code>A</code>“ 以及包含 “<code>Z</code>“ 的 “<code>cache</code>“（它会作为输入传给对应的反向传播函数）。使用方式如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A, activation_cache = sigmoid(Z)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>ReLU</strong>：ReLU 的数学形式为 $A &#x3D; RELU(Z) &#x3D; max(0, Z)$。已实现 <code>relu</code> 函数（见附录）。该函数同样会返回 <strong>两个</strong> 值：激活值 “<code>A</code>“ 以及包含 “<code>Z</code>“ 的 “<code>cache</code>“（它会作为输入传给对应的反向传播函数）。使用方式如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A, activation_cache = relu(Z)</span><br></pre></td></tr></table></figure>

<p> <strong>LINEAR-&gt;ACTIVATION</strong> 层的前向传播。数学关系为：<br>$$<br>A^{[l]} &#x3D; g(Z^{[l]}) &#x3D; g(W^{[l]}A^{[l-1]} +b^{[l]})\tag{2}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_activation_forward</span>(<span class="params">A_prev, W, b, activation</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现 LINEAR-&gt;ACTIVATION 层的前向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    A_prev -- 来自前一层的激活值（或输入数据）：(前一层大小, 样本数)</span></span><br><span class="line"><span class="string">    W -- 权重矩阵：形状为 (当前层大小, 前一层大小) 的 numpy 数组</span></span><br><span class="line"><span class="string">    b -- 偏置向量：形状为 (当前层大小, 1) 的 numpy 数组</span></span><br><span class="line"><span class="string">    activation -- 本层使用的激活函数，字符串：&quot;sigmoid&quot; 或 &quot;relu&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    A -- 激活函数的输出（也称为 post-activation 值）</span></span><br><span class="line"><span class="string">    cache -- 一个 Python 元组，包含 &quot;linear_cache&quot; 和 &quot;activation_cache&quot;；</span></span><br><span class="line"><span class="string">             用于高效计算反向传播</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    linear_cache = (A_prev,W,b)</span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>

<h3 id="L层模型"><a href="#L层模型" class="headerlink" title="L层模型"></a>L层模型</h3><p>- [LINEAR -&gt; RELU] $\times$ (L-1) -&gt; LINEAR -&gt; SIGMOID（整个模型）</p>
<p>在下面的代码中，变量 <em><code>AL</code></em> 表示 $A^{[L]} &#x3D; \sigma(Z^{[L]}) &#x3D; \sigma(W^{[L]} A^{[L-1]} + b^{[L]})$。（有时也被称为 $\hat{Y}$）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_model_forward</span>(<span class="params">X, parameters</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现 [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID 的前向传播计算</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    X -- 输入数据，形状为 (输入维度, 样本数) 的 numpy 数组</span></span><br><span class="line"><span class="string">    parameters -- initialize_parameters_deep() 的输出</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    AL -- 最后一层的 post-activation 值</span></span><br><span class="line"><span class="string">    caches -- cache 列表，包含：</span></span><br><span class="line"><span class="string">                每一次 linear_relu_forward() 的 cache（共有 L-1 个，下标从 0 到 L-2）</span></span><br><span class="line"><span class="string">                linear_sigmoid_forward() 的 cache（只有 1 个，下标为 L-1）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span>                  <span class="comment"># 神经网络的层数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 [LINEAR -&gt; RELU]*(L-1)。将 &quot;cache&quot; 添加到 &quot;caches&quot; 列表中。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">&#x27;W&#x27;</span>+<span class="built_in">str</span>(i)], parameters[<span class="string">&#x27;b&#x27;</span>+<span class="built_in">str</span>(i)], <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 LINEAR -&gt; SIGMOID。将 &quot;cache&quot; 添加到 &quot;caches&quot; 列表中。</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">&#x27;W&#x27;</span>+<span class="built_in">str</span>(L)], parameters[<span class="string">&#x27;b&#x27;</span>+<span class="built_in">str</span>(L)], <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure>

<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>使用下面的公式计算交叉熵代价 $J$：<br>$$<br>-\frac{1}{m} \sum_{i &#x3D; 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L] (i)}\right)) \tag{3}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">AL, Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现代价函数（对应公式 (3)）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    AL -- 与标签预测对应的概率向量，形状 (1, 样本数)</span></span><br><span class="line"><span class="string">    Y -- 真实“标签”向量（例如：非猫为 0，猫为 1），形状 (1, 样本数)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    cost -- 交叉熵代价</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据 aL 和 y 计算损失。</span></span><br><span class="line">    cost = -<span class="number">1</span>/m*np.<span class="built_in">sum</span>(Y*np.log(AL)+(<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-AL))</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># 确保 cost 的形状符合预期（例如把 [[17]] 变为 17）。</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>

<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="线性反向传播"><a href="#线性反向传播" class="headerlink" title="线性反向传播"></a>线性反向传播</h3><p>对于第 $l$ 层，线性部分为：$Z^{[l]} &#x3D; W^{[l]} A^{[l-1]} + b^{[l]}$</p>
<p>使用输入 $dZ^{[l]}$ 可以计算三个输出 $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$。需要用到以下公式：<br>$$<br>dW^{[l]} &#x3D; \frac{\partial \mathcal{L} }{\partial W^{[l]}} &#x3D; \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{4}<br>$$</p>
<p>$$<br>db^{[l]} &#x3D; \frac{\partial \mathcal{L} }{\partial b^{[l]}} &#x3D; \frac{1}{m} \sum_{i &#x3D; 1}^{m} dZ^{[l] (i)}\tag{5}<br>$$</p>
<p>$$<br>dA^{[l-1]} &#x3D; \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} &#x3D; W^{[l] T} dZ^{[l]} \tag{6}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_backward</span>(<span class="params">dZ, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现单层（第 l 层）反向传播的线性部分</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    dZ -- 代价函数相对于线性输出（当前层 l）的梯度</span></span><br><span class="line"><span class="string">    cache -- 来自当前层前向传播的缓存 (A_prev, W, b) 元组</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    dA_prev -- 代价函数相对于上一层（l-1）激活值的梯度，形状与 A_prev 相同</span></span><br><span class="line"><span class="string">    dW -- 代价函数相对于 W（当前层 l）的梯度，形状与 W 相同</span></span><br><span class="line"><span class="string">    db -- 代价函数相对于 b（当前层 l）的梯度，形状与 b 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    dW = <span class="number">1</span>/m*np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span>/m*np.<span class="built_in">sum</span>(dZ,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>

<h3 id="线性-激活反向传播"><a href="#线性-激活反向传播" class="headerlink" title="线性-激活反向传播"></a>线性-激活反向传播</h3><p>预先提供的两个反向函数（见附录）</p>
<ul>
<li><strong><code>sigmoid_backward</code></strong>：实现 SIGMOID 单元的反向传播。调用方式：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dZ = sigmoid_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>relu_backward</code></strong>：实现 RELU 单元的反向传播。调用方式：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dZ = relu_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_activation_backward</span>(<span class="params">dA, cache, activation</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现 LINEAR-&gt;ACTIVATION 层的反向传播。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    dA -- 当前层 l 的 post-activation 梯度</span></span><br><span class="line"><span class="string">    cache -- 我们为高效反向传播而保存的缓存 (linear_cache, activation_cache) 元组</span></span><br><span class="line"><span class="string">    activation -- 本层使用的激活函数，字符串：&quot;sigmoid&quot; 或 &quot;relu&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    dA_prev -- 代价函数相对于上一层（l-1）激活值的梯度，形状与 A_prev 相同</span></span><br><span class="line"><span class="string">    dW -- 代价函数相对于 W（当前层 l）的梯度，形状与 W 相同</span></span><br><span class="line"><span class="string">    db -- 代价函数相对于 b（当前层 l）的梯度，形状与 b 相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ,linear_cache)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ,linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>

<h3 id="L层模型-1"><a href="#L层模型-1" class="headerlink" title="L层模型"></a>L层模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_model_backward</span>(<span class="params">AL, Y, caches</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现 [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID 这一整体结构的反向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    AL -- 概率向量，前向传播（L_model_forward()）的输出</span></span><br><span class="line"><span class="string">    Y -- 真实“标签”向量（非猫为 0，猫为 1）</span></span><br><span class="line"><span class="string">    caches -- cache 列表，包含：</span></span><br><span class="line"><span class="string">                每一层 activation 为 &quot;relu&quot; 的 linear_activation_forward() 的 cache（即 caches[l]，其中 l = 0...L-2）</span></span><br><span class="line"><span class="string">                activation 为 &quot;sigmoid&quot; 的 linear_activation_forward() 的 cache（即 caches[L-1]）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    grads -- 包含梯度的字典：</span></span><br><span class="line"><span class="string">             grads[&quot;dA&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads[&quot;dW&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads[&quot;db&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(caches)</span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line">    <span class="comment"># 初始化反向传播</span></span><br><span class="line">    dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    <span class="comment"># 注意编号方式不同，cache下标为0~L-1，dA|dW|db下标为1~L</span></span><br><span class="line">    current_cache = caches[L - <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 第 L 层（SIGMOID -&gt; LINEAR）的梯度</span></span><br><span class="line">    grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(L)] = linear_activation_backward(dAL, current_cache, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(L - <span class="number">1</span>)):</span><br><span class="line">        <span class="comment"># 第 l 层：（RELU -&gt; LINEAR）梯度</span></span><br><span class="line">        current_cache = caches[i]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(i + <span class="number">2</span>)], current_cache, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = db_temp</span><br><span class="line">    <span class="comment"># eg.L=4时，i的范围为2、1、0，对应3、2、1层，第L层需要用到下一层传回来的梯度    </span></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>

<h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><p>使用梯度下降来更新模型参数：<br>$$<br>W^{[l]} &#x3D; W^{[l]} - \alpha \text{ } dW^{[l]} \tag{7}<br>$$</p>
<p>$$<br>b^{[l]} &#x3D; b^{[l]} - \alpha \text{ } db^{[l]} \tag{8}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用梯度下降更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    parameters -- 包含参数的 Python 字典</span></span><br><span class="line"><span class="string">    grads -- 包含梯度的 Python 字典（L_model_backward 的输出）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    parameters -- 更新后的参数字典：</span></span><br><span class="line"><span class="string">                  parameters[&quot;W&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">                  parameters[&quot;b&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># 神经网络的层数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对每个参数应用更新规则。使用 for 循环。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,L+<span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span>+<span class="built_in">str</span>(i)] = parameters[<span class="string">&#x27;W&#x27;</span>+<span class="built_in">str</span>(i)] - learning_rate* grads[<span class="string">&quot;dW&quot;</span>+<span class="built_in">str</span>(i)]</span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span>+<span class="built_in">str</span>(i)] = parameters[<span class="string">&#x27;b&#x27;</span>+<span class="built_in">str</span>(i)] - learning_rate* grads[<span class="string">&quot;db&quot;</span>+<span class="built_in">str</span>(i)]</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>



<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">Z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements the sigmoid activation in numpy</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z -- numpy array of any shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of sigmoid(z), same shape as Z</span></span><br><span class="line"><span class="string">    cache -- returns Z as well, useful during backpropagation</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    A = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    cache = Z</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">Z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the RELU function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z -- Output of the linear layer, of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- Post-activation parameter, of the same shape as Z</span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing &quot;A&quot; ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    cache = Z </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu_backward</span>(<span class="params">dA, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a single RELU unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient, of any shape</span></span><br><span class="line"><span class="string">    cache -- &#x27;Z&#x27; where we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    dZ = np.array(dA, copy=<span class="literal">True</span>) <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># When z &lt;= 0, you should set dz to 0 as well. </span></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_backward</span>(<span class="params">dA, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a single SIGMOID unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient, of any shape</span></span><br><span class="line"><span class="string">    cache -- &#x27;Z&#x27; where we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (<span class="number">1</span>-s)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Self-Attention</title>
    <url>//2026/02/14/Self-Attention/</url>
    <content><![CDATA[<h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h1><h2 id="核心公式"><a href="#核心公式" class="headerlink" title="核心公式"></a>核心公式</h2><p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$</p>
<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p><img src="https://pic1.imgdb.cn/item/6990880a92498366b1aa00c0.png" alt="自注意力模型的运作方式"></p>
<p>我们要通过考虑一整串输入的向量来得到输出，每个$b$都是考虑了所有$a$之后产生的。以$b^1$为例，我们来介绍这个过程。</p>
<p>自注意力的目的是考虑整个序列，但是又不希望把整个序列的所有信息包在一个窗口里，所以有一个特别的机制，用$\alpha$来判断每个向量与$a^1$的关联程度</p>
<p><img src="https://pic1.imgdb.cn/item/6990880a92498366b1aa00bc.png" alt="计算向量关联程度的方法"></p>
<p>两个向量点积的结果在某种程度上可以反映两个向量的相似度。如图(a)方式，把输入的两个向量分别乘上两个不同的矩阵$W^q$、$W^k$，得到两个向量$q$和$k$，再把$q$和$k$做点积，把他们做逐元素的相乘，求和后就得到了$\alpha$。这是计算$\alpha$的一种方法，也是目前最常用的一种方法，另一种计算方式见图(b)</p>
<p>将各个向量的关联性计算出之后（一般自己与自己也会计算关联性）,除以$\sqrt{d_k}$(防止$QK^T$数值过大softmax梯度消失)，对所有的关联性做一个softmax操作，即$\alpha\prime_{1,i} &#x3D; e^{\alpha_{1,i}} &#x2F; \sum_{j} e^{\alpha_{1,j}}$ ,得到 $a\prime$</p>
<p><img src="https://pic1.imgdb.cn/item/6990880a92498366b1aa00be.png" alt="添加softmax"></p>
<p>接下来我们要根据$\alpha’$，去抽取出序列里面重要的信息。把向量$a^1$到$a^4$乘上$W^v$得到新的向量$v^1$到$v^4$，乘上对应的注意力分数$\alpha’$，求和，即$b^1&#x3D;\sum_ia’_{1,i}v^i$,就得到了对应的b</p>
<p><img src="https://pic1.imgdb.cn/item/6990880a92498366b1aa00bf.png" alt="根据α′抽取序列中重要的信息"><br>将上述过程向量化后，即得到了Self-Attention的核心公式<br><img src="https://pic1.imgdb.cn/item/6990880a92498366b1aa00bd.png" alt="从矩阵乘法的角度来理解注意力"></p>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>需要考虑位置信息时，就要用到位置编码。位置编码为每一个位置设定一个位置向量，用$e^i$表示，上标$i$代表位置，不同的位置就有不同的向量<img src="https://pic1.imgdb.cn/item/6990880a92498366b1aa00bb.png" alt="位置编码"></p>
<h2 id="进阶版本"><a href="#进阶版本" class="headerlink" title="进阶版本"></a>进阶版本</h2><p>多头自注意力（multi-head self-attention）</p>
<p>相关有很多种不同的形式，所以也许可以有多个$q$，不同的$q$负责不同种类的相关性<img src="https://pic1.imgdb.cn/item/6990887c92498366b1aa00c5.png" alt="多头自注意力的计算过程"></p>
<p>截断自注意力（truncated self-attention）可以处理向量序列长度过大的问题。<img src="https://pic1.imgdb.cn/item/6990887c92498366b1aa00c6.png" alt="截断自注意力"></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络RNN部分原理</title>
    <url>//2026/02/15/RNN/</url>
    <content><![CDATA[<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><h2 id="什么是RNN？"><a href="#什么是RNN？" class="headerlink" title="什么是RNN？"></a>什么是RNN？</h2><p>隐藏层的输出会被保留在记忆元中，在处理下一次的输入时，记忆元中的值也会被考虑。它会考虑序列的顺序，输入序列的顺序不同输出也不同。下图为权重为1，偏置为0，激活函数为线性的RNN架构处理输入序列$[1,1]^T,[1,1]^T,[2,2]^T$的情况</p>
<p><img src="/2026/02/15/RNN/image-20260215111432342.png" alt="循环神经网络运算示例"></p>
<h2 id="其他RNN"><a href="#其他RNN" class="headerlink" title="其他RNN"></a>其他RNN</h2><p>循环神经网络的架构是可以任意设计的，之前提到的RNN只有一个隐藏层，但RNN也可以是深层的。</p>
<p><img src="/2026/02/15/RNN/image-20260215112252727.png" alt="深层循环神经网络"></p>
<h3 id="Elman-网络-Jordan-网络"><a href="#Elman-网络-Jordan-网络" class="headerlink" title="Elman 网络 &amp;Jordan 网络"></a>Elman 网络 &amp;Jordan 网络</h3><p>Jordan 网络存的是整个网络输出的值，它把输出值在下一个时间点在读进来，把输出存到记忆元里。Elman网络没有目标，很难控制说它能学到什么隐藏层信息（学到什么放到记忆元里），但是Jordan网络是有目标，比较很清楚记忆元存储的东西。</p>
<p><img src="/2026/02/15/RNN/image-20260215112312814.png" alt="Elman 网络和 Jordan 网络"></p>
<h3 id="双向循环神经网络Bi-RNN"><a href="#双向循环神经网络Bi-RNN" class="headerlink" title="双向循环神经网络Bi-RNN"></a>双向循环神经网络Bi-RNN</h3><p>同时训练一个正向的循环神经网络，训练一个逆向的循环神经网络，然后把这两个循环神经网络的隐藏层拿出来，都接给一个输出层得到最后的$y_t$。它的好处是神经元产生输出的时候，考虑的范围是比较广的<img src="/2026/02/15/RNN/image-20260215112742532.png" alt="双向循环神经网络"></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM有三个<strong>门</strong>，负责控制是否输入、清除记忆和输出</p>
<p>**输入门：**输入门被打开时，才能把输入值写入记忆元</p>
<p>**输出门：**输出门被打开时，外界的神经元才能把记忆元的内容读取出来</p>
<p>**遗忘门：**遗忘门决定什么时候将记忆元的东西遗忘。当遗忘门被打开时，将输入与记忆元内容进行计算后存入记忆元。当遗忘门被关闭时，清除记忆元的内容。记忆元对应的计算公式为$c\prime &#x3D; g(z)f(z_i)+cf(z_f)$</p>
<p><img src="/2026/02/15/RNN/image-20260215114426951.png" alt="LSTM 记忆元示例"></p>
<p>假设要被存到单元的输入为$z$，操控输入门的信号为$z_i$，操控遗忘门的信号为$z_f$，操控输出门为$z_o$，综合这些会得到一个输出记为$a$。</p>
<p>$z$通过激活函数得到$g(z)$，$z_i$通过激活函数f得到$f(z_i)$，f通常会选择sigmoid函数，因为其值介于0~1之间，可以表示门被打开的程度，如果f的输出是1，表示门被打开的状态，反之这个门是关起来的。接下来把$g(z)$乘$f(z_i)$得到$g(z)f(z_i)$。对于遗忘门的$z_f$，也是通过sigmoid函数得到$f(z_f)$，接下来把记忆元里的值$c$乘$f(z_f)$得到$cf(z_f)$，相加得到$c’&#x3D;g(z)f(z_i)+cf(z_f)$存入记忆元中。</p>
<p><strong>LSTM原理</strong></p>
<p>原来的神经网络里会有很多神经元，可以直接把LSTM当作一个神经元</p>
<p><img src="/2026/02/15/RNN/image-20260215171137415.png" alt="LSTM 结构示意"></p>
<p>假设用的神经元的数量跟LSTM是一样的，则LSTM需要的参数量是一般神经网络的四倍。</p>
<p><img src="/2026/02/15/RNN/image-20260215171653840.png" alt="输入向量与记忆元的关系"></p>
<p>如图，假设有一排LSTM，将所有LSTM的值连接起来就变成了向量，记作$c^{t-1}$，现在在时间节点$t$，输入向量$x_t$，这个向量首先会乘上一矩阵变成一个向量$z$。向量$z$就代表了每个LSTM的输入，z的维度刚好是LSTM的数量。将$x_t$进行另外一次变换得到$z_i$，$z_i$的维度也和LSTM单元的数量一样，$z_i$的每一个维度都会去操控输入门。输出门与遗忘门也是相同的道理。所以我们把$x_t$进行四次不同的变换得到四个不同的向量，四个向量的维度与LSTM的数量一样，把这些向量合起来就会去操控这些记忆元的运作。<img src="/2026/02/15/RNN/image-20260215172653498.png" alt="记忆元一起运算示例"></p>
<p>完成该时间点的操作后，继续进行下一个时间点的运算。真正的LSTM还会把上个时间点输出值$h^t$接入当作下一个时间点的输入，并添加peephole连接，即把记忆元的值也拉过来。操控LSTM四个门的时候，同时考虑$x_{t+1},h^t,c^t$，把这三个向量并在一起经过变换得到四个不同的向量再去操控LSTM</p>
<p><img src="/2026/02/15/RNN/image-20260215174132880.png" alt="LSTM完全体"></p>
<p>LSTM 通常不会只有一层</p>
<p><img src="/2026/02/15/RNN/image-20260215174306377.png" alt="多层LSTM">门控循环单元GRU是LSTM的简化版本，它只有两个门，但性能相近，少了1&#x2F;3的参数，不容易过拟合。</p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>DL基础</title>
    <url>//2026/02/13/baseDL/</url>
    <content><![CDATA[<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>$g’(z)&#x3D;g(z)(1-g(z))$<img src="https://pic1.imgdb.cn/item/698f3affe887663d376fdb91.png" alt="sigmoid"></p>
<p>tanh：$g(z)&#x3D;tanh(z)&#x3D;a&#x3D;\frac{e^z-e^{-z}}{e^z+e^{-z}}$,$g’(z)&#x3D;(1-g^2(z))$<img src="https://pic1.imgdb.cn/item/698f3affe887663d376fdb8f.png" alt="tanh"></p>
<p>ReLU:$g(z)&#x3D;a&#x3D;max(0,z)$<img src="https://pic1.imgdb.cn/item/698f3affe887663d376fdb90.png" alt="ReLU"></p>
<p>leaky ReLU:$a&#x3D;max(0.01z,z)$<img src="https://pic1.imgdb.cn/item/698f3afee887663d376fdb8e.png" alt="leaky ReLU"></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>常见的损失函数有均方误差和交叉熵误差</p>
<h3 id="均方误差-MSE"><a href="#均方误差-MSE" class="headerlink" title="均方误差(MSE)"></a>均方误差(MSE)</h3><p><strong>公式</strong>：$E &#x3D; \frac{1}{2} \sum_{k}(y_k - t_k)^2$，其中$y_k$表示神经网络的输出，$t_k$表示监督数据，k表示数据的维数。</p>
<p><strong>python实现</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.<span class="built_in">sum</span>((y-t)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h3><p><strong>公式</strong>：$E &#x3D; -\sum_{k} t_k \log y_k$</p>
<p><strong>python实现：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    delta = <span class="number">1e-7</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + delta))</span><br></pre></td></tr></table></figure>

<p>这里，参数y和t是NumPy数组。函数内部在计算np.log时，加上了一个微小值delta。这是因为，当出现np.log(0)时，np.log(0)会变为负无限大的-inf，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。</p>
<h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><p>将正确解标签表示为1，其他标签表示为0的表示方法称为<strong>one-hot编码</strong></p>
<h2 id="mini-batch"><a href="#mini-batch" class="headerlink" title="mini-batch"></a>mini-batch</h2><p>神经网络的学习也是从训练数据中选出一批数据，然后对每个mini-batch进行学习。比如，从60000个训练数据中随机选择100笔，再用这100笔数据进行学习。这种学习方式称为<strong>mini-batch学习</strong>，同时，mini-batch的损失函数也是利用 一部分样本数据来近似地计算整体</p>
<h3 id="mini-batch版交叉熵误差的实现"><a href="#mini-batch版交叉熵误差的实现" class="headerlink" title="mini-batch版交叉熵误差的实现"></a>mini-batch版交叉熵误差的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + <span class="number">1e-7</span>)) / batch_size</span><br></pre></td></tr></table></figure>

<p>当监督数据是标签形式（非one-hot表示，而是像“2”“ 7”这样的标签）时，交叉熵误差可通过如下代码实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="number">1e-7</span>)) / batch_size</span><br></pre></td></tr></table></figure>

<p>实现的要点是，由于one-hot表示中t为0的元素的交叉熵误差也为0，因 此针对这些元素的计算可以忽略。换言之，如果可以获得神经网络在正确解标签处的输出，就可以计算交叉熵误差。因此，t为one-hot表示时通过 t * np.log(y) 计算的地方，在t为标签形式时，可用np.log( y[np.arange  (batch_size), t] )实现相同的处理</p>
<h2 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h2><p>通过使用梯度来寻找函数最小值（或者尽可能小的值）的方法，即通过不断地沿梯度方向前进， 逐渐减小函数值的过程，常见的参数更新公式$x_0 &#x3D; x_0 - \eta \frac{\partial f}{\partial x_0}$、$x_1 &#x3D; x_1 - \eta \frac{\partial f}{\partial x_1}$就来自于此</p>
<p>通过梯度下降法更新参数，并且当使用的数据是随机选择的mini batch数据时，称该方法为随机梯度下降法(SGD)</p>
<h2 id="神经网络的学习过程"><a href="#神经网络的学习过程" class="headerlink" title="神经网络的学习过程"></a>神经网络的学习过程</h2><h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><p>神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤。</p>
<h3 id="步骤1（mini-batch）"><a href="#步骤1（mini-batch）" class="headerlink" title="步骤1（mini-batch）"></a>步骤1（mini-batch）</h3><p>从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值。 </p>
<h3 id="步骤2（计算梯度）"><a href="#步骤2（计算梯度）" class="headerlink" title="步骤2（计算梯度）"></a>步骤2（计算梯度）</h3><p>为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。 梯度表示损失函数的值减小最多的方向。</p>
<h3 id="步骤3（更新参数）"><a href="#步骤3（更新参数）" class="headerlink" title="步骤3（更新参数）"></a>步骤3（更新参数）</h3><p>将权重参数沿梯度方向进行微小更新。</p>
<h3 id="步骤4（重复）"><a href="#步骤4（重复）" class="headerlink" title="步骤4（重复）"></a>步骤4（重复）</h3><p>重复步骤1、步骤2、步骤3。</p>
<h2 id="Logistic-Regression-cost-function"><a href="#Logistic-Regression-cost-function" class="headerlink" title="Logistic Regression cost function"></a>Logistic Regression cost function</h2><p>$$<br>\hat{y}&#x3D;\sigma(w^Tx+b),where\ \sigma(z)&#x3D;\frac{1}{1+e^{-z}}<br>$$</p>
<p>Loss function:通常采用$L(\hat{y},y)&#x3D;\frac{1}{2}(\hat{y}-y)^2$作为loss function，在逻辑回归中不这么做，因为其对梯度下降法很可能找不到全局最优值</p>
<p>我们采用的是对数损失函数$L(\hat{y},y)&#x3D;-(ylog\hat{y}+(1-y)log(1-\hat{y}))$，它是一个凸函数</p>
<p>Cost function:$J(w,b)&#x3D;\frac{1}{m}\sum{_{i&#x3D;1}^mL(\hat{y}^{(i)},y^{(i)})}$</p>
<h2 id="层的思想在神经网络中的应用"><a href="#层的思想在神经网络中的应用" class="headerlink" title="层的思想在神经网络中的应用"></a>层的思想在神经网络中的应用</h2><p><strong>乘法层</strong>：正向传播计算乘积。反向传播会将上游传来的导数乘以“翻转值”（即正向传播时的另一个输入值）。需要保存正向传播时的输入变量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MulLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.x = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.y = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="variable language_">self</span>.x = x</span><br><span class="line">        <span class="variable language_">self</span>.y = y</span><br><span class="line">        <span class="keyword">return</span> x * y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        <span class="comment"># 翻转x和y</span></span><br><span class="line">        dx = dout * <span class="variable language_">self</span>.y</span><br><span class="line">        dy = dout * <span class="variable language_">self</span>.x</span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br></pre></td></tr></table></figure>

<p><strong>加法层</strong>：正向传播计算和。反向传播将上游导数原封不动地传递给下游（乘以1）。不需要保存输入变量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AddLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="keyword">return</span> x + y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * <span class="number">1</span></span><br><span class="line">        dy = dout * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br></pre></td></tr></table></figure>

<p><strong>激活函数层</strong>：</p>
<p>ReLU：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Relu</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.mask = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="variable language_">self</span>.mask = (x &lt;= <span class="number">0</span>)</span><br><span class="line">        out = x.copy()</span><br><span class="line">        out[<span class="variable language_">self</span>.mask] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dout[<span class="variable language_">self</span>.mask] = <span class="number">0</span></span><br><span class="line">        dx = dout</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>

<p>Sigmoid：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Sigmoid</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">        <span class="variable language_">self</span>.out = out</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * (<span class="number">1.0</span> - <span class="variable language_">self</span>.out) * <span class="variable language_">self</span>.out</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>

<p>Affine：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Affine</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, W, b</span>):</span><br><span class="line">        <span class="variable language_">self</span>.W = W</span><br><span class="line">        <span class="variable language_">self</span>.b = b</span><br><span class="line">        <span class="variable language_">self</span>.x = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.dW = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.db = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="variable language_">self</span>.x = x</span><br><span class="line">        out = np.dot(x, <span class="variable language_">self</span>.W) + <span class="variable language_">self</span>.b</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = np.dot(dout, <span class="variable language_">self</span>.W.T)</span><br><span class="line">        <span class="variable language_">self</span>.dW = np.dot(<span class="variable language_">self</span>.x.T, dout)</span><br><span class="line">        <span class="variable language_">self</span>.db = np.<span class="built_in">sum</span>(dout, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>

<p>Softmax-with-Loss：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxWithLoss</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.loss = <span class="literal">None</span> <span class="comment"># 损失</span></span><br><span class="line">        <span class="variable language_">self</span>.y = <span class="literal">None</span>    <span class="comment"># softmax的输出</span></span><br><span class="line">        <span class="variable language_">self</span>.t = <span class="literal">None</span>    <span class="comment"># 监督数据（one-hot vector）</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        <span class="variable language_">self</span>.t = t</span><br><span class="line">        <span class="variable language_">self</span>.y = softmax(x)</span><br><span class="line">        <span class="variable language_">self</span>.loss = cross_entropy_error(<span class="variable language_">self</span>.y, <span class="variable language_">self</span>.t)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.loss</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span><br><span class="line">        batch_size = <span class="variable language_">self</span>.t.shape[<span class="number">0</span>]</span><br><span class="line">        dx = (<span class="variable language_">self</span>.y - <span class="variable language_">self</span>.t) / batch_size</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>

<p>应用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> common.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> common.gradient <span class="keyword">import</span> numerical_gradient</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size,</span></span><br><span class="line"><span class="params">                 weight_init_std=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">初始化权重</span><br><span class="line">        <span class="variable language_">self</span>.params = &#123;&#125;</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&#x27;W1&#x27;</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(input_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&#x27;W2&#x27;</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(hidden_size, output_size)</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">生成层</span><br><span class="line">        <span class="variable language_">self</span>.layers = OrderedDict()</span><br><span class="line">        <span class="variable language_">self</span>.layers[<span class="string">&#x27;Affine1&#x27;</span>] = \</span><br><span class="line">            Affine(<span class="variable language_">self</span>.params[<span class="string">&#x27;W1&#x27;</span>], <span class="variable language_">self</span>.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.layers[<span class="string">&#x27;Relu1&#x27;</span>] = Relu()</span><br><span class="line">        <span class="variable language_">self</span>.layers[<span class="string">&#x27;Affine2&#x27;</span>] = \</span><br><span class="line">            Affine(<span class="variable language_">self</span>.params[<span class="string">&#x27;W2&#x27;</span>], <span class="variable language_">self</span>.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.lastLayer = SoftmaxWithLoss()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers.values():</span><br><span class="line">            x = layer.forward(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="comment"># x: 输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = <span class="variable language_">self</span>.predict(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.lastLayer.forward(y, t)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = <span class="variable language_">self</span>.predict(x)</span><br><span class="line">        y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> t.ndim != <span class="number">1</span> : t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy = np.<span class="built_in">sum</span>(y == t) / <span class="built_in">float</span>(x.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line">    <span class="comment"># x: 输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        loss_W = <span class="keyword">lambda</span> W: <span class="variable language_">self</span>.loss(x, t)</span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, <span class="variable language_">self</span>.params[<span class="string">&#x27;W1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, <span class="variable language_">self</span>.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, <span class="variable language_">self</span>.params[<span class="string">&#x27;W2&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, <span class="variable language_">self</span>.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        <span class="variable language_">self</span>.loss(x, t)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        dout = <span class="number">1</span></span><br><span class="line">        dout = <span class="variable language_">self</span>.lastLayer.backward(dout)</span><br><span class="line">        layers = <span class="built_in">list</span>(<span class="variable language_">self</span>.layers.values())</span><br><span class="line">        layers.reverse()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="comment"># 设定</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = <span class="variable language_">self</span>.layers[<span class="string">&#x27;Affine1&#x27;</span>].dW</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = <span class="variable language_">self</span>.layers[<span class="string">&#x27;Affine1&#x27;</span>].db</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = <span class="variable language_">self</span>.layers[<span class="string">&#x27;Affine2&#x27;</span>].dW</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = <span class="variable language_">self</span>.layers[<span class="string">&#x27;Affine2&#x27;</span>].db</span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>面向竞赛的C++STL</title>
    <url>/2025/01/25/Basic-STL/</url>
    <content><![CDATA[<h1 id="面向竞赛的-C-STL-介绍"><a href="#面向竞赛的-C-STL-介绍" class="headerlink" title="面向竞赛的 C++ STL 介绍"></a>面向竞赛的 C++ STL 介绍</h1><p>参考链接：<br><a href="https://space.bilibili.com/231911980/lists/4220963?type=season">https://space.bilibili.com/231911980/lists/4220963?type=season</a><br><a href="https://space.bilibili.com/498363953/lists/241249?type=season">https://space.bilibili.com/498363953/lists/241249?type=season</a><br><a href="https://blog.csdn.net/weixin_51797626/article/details/123317216?spm=1001.2014.3001.5501">https://blog.csdn.net/weixin_51797626/article/details/123317216?spm=1001.2014.3001.5501</a></p>
<h2 id="vector"><a href="#vector" class="headerlink" title="vector"></a>vector</h2><p><strong>涉及函数:at,back,push_back,pop_back,size,erase,clear,begin,rbegin,end,resize</strong></p>
<ul>
<li>创建数组</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; v<span class="comment">//创建int类型的空数组v</span></span><br><span class="line">v.<span class="built_in">resize</span>(n)<span class="comment">//创建后可随时自定义大小</span></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">v</span><span class="params">(<span class="number">10</span>)</span><span class="comment">//创建int类型的数组v，包含10个元素，所有元素默认初始化为0</span></span></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">v</span><span class="params">(<span class="number">10</span>,<span class="number">66</span>)</span><span class="comment">//创建int类型的数组v，包含10个元素，所有元素默认初始化为66</span></span></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; v</span>=&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>&#125;<span class="comment">//通过列表初始化创建</span></span><br></pre></td></tr></table></figure>

<ul>
<li>访问元素</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> x=v[<span class="number">1</span>];</span><br><span class="line"><span class="type">int</span> y=v.<span class="built_in">at</span>(<span class="number">1</span>);<span class="comment">//会进行边界检查，可以防止越界</span></span><br></pre></td></tr></table></figure>

<ul>
<li>修改元素</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">v[<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">v.<span class="built_in">push_back</span>(<span class="number">1</span>);<span class="comment">//在数组最后添加元素1</span></span><br><span class="line">v.<span class="built_in">pop_back</span>();<span class="comment">//删除数组最后一个元素</span></span><br><span class="line">v.<span class="built_in">clear</span>();<span class="comment">//清空</span></span><br><span class="line">v.<span class="built_in">erase</span>(v.<span class="built_in">begin</span>()+n);<span class="comment">//删除第n+1个元素</span></span><br></pre></td></tr></table></figure>

<ul>
<li>遍历元素</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//常规</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;v.<span class="built_in">size</span>();i++)</span><br><span class="line">    cout&lt;&lt;v[i]&lt;&lt;<span class="string">&#x27; &#x27;</span>;</span><br><span class="line"><span class="comment">//范围基</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> num:v)</span><br><span class="line">    cout&lt;&lt;num&lt;&lt;<span class="string">&#x27; &#x27;</span>;</span><br><span class="line"><span class="comment">//pair/结构体</span></span><br><span class="line">vector&lt;pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt;&gt; v_pair;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">const</span> <span class="keyword">auto</span>&amp; [a.b]:v_pair)</span><br><span class="line">    cout&lt;&lt;a&lt;&lt;<span class="string">&#x27; &#x27;</span>&lt;&lt;b&lt;&lt;<span class="string">&#x27; &#x27;</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询大小</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> size=v.<span class="built_in">size</span>()<span class="comment">//返回unsigned int，&gt;=0</span></span><br><span class="line"><span class="keyword">if</span>(v.<span class="built_in">empty</span>())...<span class="comment">//检查v是否为空</span></span><br><span class="line"><span class="keyword">if</span>(v.<span class="built_in">size</span>())...<span class="comment">//检查v是否为空</span></span><br></pre></td></tr></table></figure>
<h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a><a href="https://www.starrycoding.com/problem/330">例题</a></h4><hr>
<h2 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h2><p><strong>涉及函数:push,top,pop,size,empty</strong><br><strong>常用于:单调栈，括号匹配，dfs，tarjan 求强连通分量，波兰表达式(计算器)</strong></p>
<ul>
<li>初始化</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">stack &lt;<span class="type">int</span>&gt; stk;<span class="comment">//栈不允许列表初始化和填充相同元素</span></span><br><span class="line">stack &lt;<span class="type">int</span>&gt; <span class="built_in">stk2</span>(stk);</span><br><span class="line">stack &lt;<span class="type">int</span>&gt; stk3=stk2;</span><br></pre></td></tr></table></figure>

<ul>
<li>出入栈<blockquote>
<p>如果你问一个人 push 的反义词是什么，如果 TA 说是 pop，那么 TA 一定是 c 艹选手</p>
</blockquote>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">stk.<span class="built_in">push</span>(<span class="number">10</span>);<span class="comment">//[10(top)]</span></span><br><span class="line">stk.<span class="built_in">push</span>(<span class="number">20</span>);<span class="comment">//[10,20(top)]</span></span><br><span class="line">stk.<span class="built_in">push</span>(<span class="number">50</span>);<span class="comment">//[10,20,50(top)]</span></span><br><span class="line">cout&lt;&lt;stk.<span class="built_in">top</span>()&lt;&lt;<span class="string">&#x27;\n&#x27;</span><span class="comment">//50,[10,,20,50(top)]</span></span><br><span class="line">stk.<span class="built_in">pop</span>()<span class="comment">//[10,20(top)],使用pop前判断非空</span></span><br><span class="line"><span class="keyword">if</span>(stk.<span class="built_in">size</span>())stk.<span class="built_in">pop</span>();</span><br><span class="line"><span class="keyword">if</span>(!stk.<span class="built_in">empty</span>())stk.pop;</span><br></pre></td></tr></table></figure>

<ul>
<li>清空栈</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(stk.<span class="built_in">size</span>())stk.<span class="built_in">pop</span>();<span class="comment">//时间复杂度O(n)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>数组模拟</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//以下标1为栈底，变量top为栈顶</span></span><br><span class="line"><span class="type">int</span> stk[N],top=<span class="number">0</span>;</span><br><span class="line"><span class="comment">//入栈x</span></span><br><span class="line">stk[++top]=x;</span><br><span class="line"><span class="comment">//出栈</span></span><br><span class="line">top--;</span><br><span class="line"><span class="comment">//取出栈顶元素</span></span><br><span class="line">cout&lt;&lt;stk[top]&lt;&lt;<span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"><span class="comment">//获取大小</span></span><br><span class="line">cout&lt;&lt;top&lt;&lt;<span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"><span class="comment">//判断是否为空</span></span><br><span class="line"><span class="keyword">if</span>(top)...</span><br><span class="line"><span class="comment">//遍历栈</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=top;i++)...</span><br></pre></td></tr></table></figure>
<h4 id="例题-1"><a href="#例题-1" class="headerlink" title="例题"></a><a href="https://www.starrycoding.com/problem/38">例题</a></h4><ul>
<li>也可用 vector,但 pop 时时间复杂度为 O(n)</li>
</ul>
<hr>
<h2 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h2><h3 id="queue"><a href="#queue" class="headerlink" title="queue"></a>queue</h3><p><strong>涉及函数:push,pop,front,back,size,empty</strong><br><strong>常用于:单调队列，模拟，约瑟夫环，bfs，分支限界搜索等</strong></p>
<ul>
<li>创建队列</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">queue&lt;<span class="type">int</span>&gt; q;</span><br></pre></td></tr></table></figure>

<ul>
<li>出入队列</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">q.<span class="built_in">push</span>(<span class="number">10</span>);</span><br><span class="line">q.<span class="built_in">pop</span>(<span class="number">10</span>);<span class="comment">//使用pop前判断非空</span></span><br></pre></td></tr></table></figure>

<ul>
<li>访问元素</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">q.<span class="built_in">front</span>();</span><br><span class="line">q.<span class="built_in">back</span>();</span><br></pre></td></tr></table></figure>
<h4 id="例题-2"><a href="#例题-2" class="headerlink" title="例题"></a><a href="https://www.starrycoding.com/problem/351">例题</a></h4><ul>
<li>检查队列大小&#x2F;是否为空（同上）</li>
</ul>
<p><code>queue</code>和<code>stack</code>一样，不允许遍历</p>
<ul>
<li>数组模拟</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> q[N]<span class="comment">//N为push的最大次数</span></span><br><span class="line"><span class="type">int</span> qh=<span class="number">1</span>,qt=<span class="number">0</span>;<span class="comment">//q_head,q_tail;[qh,qt]为有效区间</span></span><br><span class="line">q[++qt]=x;<span class="comment">//入队</span></span><br><span class="line">qh++;<span class="comment">//出队</span></span><br><span class="line">qt-qh<span class="number">+1</span><span class="comment">//大小</span></span><br></pre></td></tr></table></figure>

<h3 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//支持遍历，头尾均支持出入队</span></span><br><span class="line">deque&lt;<span class="type">int</span>&gt; dq;</span><br><span class="line">dq.<span class="built_in">push_front</span>(<span class="number">1</span>);</span><br><span class="line">dq.<span class="built_in">push_bacK</span>(<span class="number">1</span>);</span><br><span class="line"><span class="type">int</span> t = dq.<span class="built_in">front</span>();</span><br><span class="line">t = dq.<span class="built_in">back</span>();</span><br><span class="line">dq.<span class="built_in">size</span>();</span><br><span class="line">dq.<span class="built_in">empty</span>();</span><br><span class="line"><span class="comment">//弹出时先判断是否为空</span></span><br><span class="line">dq.<span class="built_in">pop_front</span>();</span><br><span class="line">dq.<span class="built_in">pop_back</span>();</span><br></pre></td></tr></table></figure>
<h4 id="例题-3"><a href="#例题-3" class="headerlink" title="例题"></a><a href="https://www.starrycoding.com/problem/357">例题</a></h4><h3 id="priority-queue"><a href="#priority-queue" class="headerlink" title="priority_queue"></a>priority_queue</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//用于每次提取队列种最大的元素或最小的元素</span></span><br><span class="line"><span class="comment">//自动排序内部数，小到大或大到小，不支持遍历</span></span><br><span class="line"></span><br><span class="line">priority_queue&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;, greater&lt;<span class="type">int</span>&gt;&gt; q1; </span><br><span class="line"><span class="comment">//greater是小根堆，用vector装载内部元素</span></span><br><span class="line">priority_queue&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;, less&lt;<span class="type">int</span>&gt;&gt; q2; </span><br><span class="line"><span class="comment">//less是大根堆</span></span><br><span class="line"></span><br><span class="line">q<span class="number">1.</span><span class="built_in">push</span>(<span class="number">3</span>), q<span class="number">2.</span><span class="built_in">push</span>(<span class="number">2</span>);</span><br><span class="line"><span class="type">int</span> t = q<span class="number">1.</span><span class="built_in">size</span>();</span><br><span class="line"><span class="keyword">while</span> (q<span class="number">1.</span><span class="built_in">size</span>())</span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> t = q<span class="number">1.</span><span class="built_in">top</span>();<span class="comment">//小根堆所以取出的数是所有数内最小的</span></span><br><span class="line">	q<span class="number">1.</span><span class="built_in">pop</span>();</span><br><span class="line">	cout &lt;&lt; t &lt;&lt; <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="例题-4"><a href="#例题-4" class="headerlink" title="例题"></a><a href="https://www.starrycoding.com/problem/361">例题</a></h4><hr>
<h2 id="string"><a href="#string" class="headerlink" title="string"></a>string</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> t=a.<span class="built_in">find</span>(<span class="string">&#x27;x&#x27;</span>);<span class="comment">//获取第一个字符x出现的位置的下标</span></span><br><span class="line">string g=a.<span class="built_in">substr</span>(<span class="number">0</span>,<span class="number">3</span>);<span class="comment">//从下标0开始(包括0)往后取三个字符</span></span><br><span class="line">stinrg g=a.<span class="built_in">substr</span>(<span class="number">3</span>)<span class="comment">//从下标3开始到结束</span></span><br></pre></td></tr></table></figure>

<h2 id="algorithm"><a href="#algorithm" class="headerlink" title="algorithm"></a>algorithm</h2><h3 id="next-permutation-prev-permutation"><a href="#next-permutation-prev-permutation" class="headerlink" title="next_permutation,prev_permutation"></a>next_permutation,prev_permutation</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//全排列函数</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="number">0</span>), cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> n, a[<span class="number">100</span>];</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        a[i] = i;</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">            cout &lt;&lt; a[i] &lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125; <span class="keyword">while</span> (<span class="built_in">next_permutation</span>(<span class="number">1</span> + a, <span class="number">1</span> + a + n));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="lower-bound"><a href="#lower-bound" class="headerlink" title="lower_bound"></a>lower_bound</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//找大于等于某数的第一个数，查找的数组必须有序</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n = <span class="number">7</span>;<span class="comment">//7个数</span></span><br><span class="line"><span class="type">int</span> a[] = &#123; <span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">12</span>,<span class="number">111</span> &#125;;</span><br><span class="line">	</span><br><span class="line"><span class="type">int</span> t = <span class="built_in">lower_bound</span>(a, a + n, <span class="number">8</span>) - a;<span class="comment">//查询范围：0 ~ 6。数组中大于等于8的第一个数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (t != n)<span class="comment">//找不到会返回边界a+n，即7</span></span><br><span class="line">	cout &lt;&lt; a[t] &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> b[] = &#123; <span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">12</span>,<span class="number">111</span> &#125;;</span><br><span class="line">t = <span class="built_in">lower_bound</span>(b + <span class="number">1</span>, b + n + <span class="number">1</span>, <span class="number">8</span>) - b;<span class="comment">//查询范围：1~8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (t != n + <span class="number">1</span>)<span class="comment">//找不到会返回边界，边界是 8</span></span><br><span class="line">	cout &lt;&lt; b[t] &lt;&lt; endl;</span><br></pre></td></tr></table></figure>

<h3 id="unique"><a href="#unique" class="headerlink" title="unique"></a>unique</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; vec = &#123; <span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">9</span> &#125;;</span><br><span class="line"><span class="built_in">sort</span>(vec.<span class="built_in">begin</span>(), vec.<span class="built_in">end</span>());</span><br><span class="line">	</span><br><span class="line"><span class="comment">//unique本身的功能是将排序后的数组内的所有重复元素在 O(n) 时间内堆积到数组末端</span></span><br><span class="line"><span class="comment">//同时它会返回一个指针/下标（区别于你传入的是容器还是数组） —— 堆积的第一个重复元素的位置</span></span><br><span class="line">	</span><br><span class="line">vec.<span class="built_in">erase</span>(<span class="built_in">unique</span>(vec.<span class="built_in">begin</span>(), vec.<span class="built_in">end</span>()), vec.<span class="built_in">end</span>());</span><br><span class="line"><span class="comment">//我们再利用vector的区间删除功能就能完成去重的过程</span></span><br></pre></td></tr></table></figure>

<h2 id="set-map"><a href="#set-map" class="headerlink" title="set,map"></a>set,map</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//可以自动排序去重</span></span><br><span class="line">set&lt;<span class="type">int</span>&gt; se;</span><br><span class="line"><span class="comment">//关键字 关键值</span></span><br><span class="line">map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; mp;<span class="comment">//map相当于拥有了关键值的set</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//插入</span></span><br><span class="line">se.<span class="built_in">insert</span>(<span class="number">3</span>);<span class="comment">//插入一个 3 关键字</span></span><br><span class="line"></span><br><span class="line">mp.<span class="built_in">insert</span>(&#123; <span class="number">1</span>,<span class="number">2</span> &#125;);<span class="comment">//插入一个 1 关键字，并给这个关键字的关键值赋值 2</span></span><br><span class="line">mp.<span class="built_in">insert</span>(&#123; <span class="number">1</span>,<span class="number">1</span> &#125;);<span class="comment">//因为 1 关键字已经存在，所以把关键值修改成 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// mp.insert(&#123; 1,2 &#125;) 等价于</span></span><br><span class="line">mp[<span class="number">1</span>] = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">mp[<span class="number">1</span>] += <span class="number">2</span>;<span class="comment">//1 关键字的关键值 + 2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//遍历</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> j : se)cout &lt;&lt; j &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> j : mp)</span><br><span class="line">	cout &lt;&lt; j.first &lt;&lt; <span class="string">&#x27; &#x27;</span> &lt;&lt; j.second &lt;&lt; endl;<span class="comment">//两个值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//删除</span></span><br><span class="line">mp.<span class="built_in">erase</span>(<span class="number">3</span>); se.<span class="built_in">erase</span>(<span class="number">3</span>);<span class="comment">//删除关键字为 3 的元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//如果是遍历删除 map 内特定的第 K 个数，跟 vector 删除一样要注意指针衔接</span></span><br><span class="line"><span class="type">int</span> k = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> it = mp.<span class="built_in">begin</span>(); it != mp.<span class="built_in">end</span>();k++) &#123;</span><br><span class="line">	<span class="keyword">if</span> (k == K)it = mp.<span class="built_in">erase</span>(it);</span><br><span class="line">	<span class="keyword">else</span> it++;</span><br><span class="line">&#125;<span class="comment">//迭代器方式</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> t = mp.<span class="built_in">count</span>(<span class="number">3</span>);<span class="comment">//返回指定元素出现的次数</span></span><br><span class="line">t = mp.<span class="built_in">size</span>();<span class="comment">//返回map中元素的个数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//查找</span></span><br><span class="line"><span class="keyword">auto</span> j = mp.<span class="built_in">find</span>(<span class="number">3</span>);<span class="comment">//查找 3 关键字在 map 内的下标</span></span><br><span class="line"><span class="comment">//如果不存在则 j == mp.end()</span></span><br><span class="line"><span class="keyword">if</span> (j != mp.<span class="built_in">end</span>())cout &lt;&lt; (*j).first;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> g = mp.<span class="built_in">lower_bound</span>(<span class="number">3</span>);<span class="comment">//查找 map 内第一个大于等于 3 的数的下标</span></span><br><span class="line"><span class="comment">//如果不存在则 g == mp.end()</span></span><br><span class="line"><span class="keyword">if</span> (g != mp.<span class="built_in">end</span>())cout &lt;&lt; (*g).first;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="pair"><a href="#pair" class="headerlink" title="pair"></a>pair</h2><p>一个变量携带两个值，可以映射数组中对应元素的下标</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; pii;</span><br><span class="line">cin &gt;&gt; pii.first &gt;&gt; pii.second;</span><br><span class="line">cout &lt;&lt; pii.first &lt;&lt; <span class="string">&#x27; &#x27;</span> &lt;&lt; pii.second;</span><br></pre></td></tr></table></figure>
<h4 id="例题-5"><a href="#例题-5" class="headerlink" title="例题"></a><a href="https://www.luogu.com.cn/problem/P1223#submit" title="排队接水">例题</a></h4><hr>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown基操</title>
    <url>/2025/01/21/Markdown/</url>
    <content><![CDATA[<p>pr一点markdown教程，学会用笔才能写好字，把文章写漂亮<br><a href="https://www.markdown.cn/">本文pr来源</a><br><a href="https://blog.csdn.net/qq_40896997/article/details/106551767">emoji</a>:blush:<br><a href="https://blog.csdn.net/wzk4869/article/details/126863936">数学表达式</a><br><a href="https://blog.csdn.net/weixin_51146207/article/details/137903410?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-137903410-blog-128811523.235%5Ev43%5Epc_blog_bottom_relevance_base7&spm=1001.2101.3001.4242.1&utm_relevant_index=3">vscode里的markdown</a></p>
<h1 id="Markdown-语法速查表"><a href="#Markdown-语法速查表" class="headerlink" title="Markdown 语法速查表"></a>Markdown 语法速查表</h1><p>Markdown 语法参考手册 &#x2F; 速查表。</p>
<h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p>此 Markdown 语法速查表提供了所有 Markdown 语法元素的快速参考。但是此速查表无法涵盖所有极限用法，因此，如果您需要某些语法元素的详细信息，请参阅我们的 <a href="https://www.markdown.xyz/basic-syntax">基本语法</a> 和 <a href="https://www.markdown.xyz/extended-syntax">扩展语法</a> 手册。</p>
<h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><p>这些是 John Gruber 的原始设计文档中列出的元素。所有 Markdown 应用程序都支持这些元素。</p>
<table>
<thead>
<tr>
<th>元素</th>
<th>Markdown 语法</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#headings">标题（Heading）</a></td>
<td><code># H1## H2### H3</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#bold">粗体（Bold）</a></td>
<td><code>**bold text**</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#italic">斜体（Italic）</a></td>
<td><code>*italicized text*</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#blockquotes-1">引用块（Blockquote）</a></td>
<td><code>&gt; blockquote</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#ordered-lists">有序列表（Ordered List）</a></td>
<td><code>1. First item2. Second item3. Third item</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#unordered-lists">无序列表（Unordered List）</a></td>
<td><code>- First item- Second item- Third item</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#code">代码（Code）</a></td>
<td><code>code</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#horizontal-rules">分隔线（Horizontal Rule）</a></td>
<td><code>---</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#links">链接（Link）</a></td>
<td><code>[title](https://www.example.com)</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/basic-syntax/#images">图片（Image）</a></td>
<td><code>![alt text](image.jpg)</code></td>
</tr>
</tbody></table>
<h2 id="扩展语法"><a href="#扩展语法" class="headerlink" title="扩展语法"></a>扩展语法</h2><p>这些元素通过添加额外的功能扩展了基本语法。但是，并非所有 Markdown 应用程序都支持这些元素。</p>
<table>
<thead>
<tr>
<th>元素</th>
<th>Markdown 语法</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#tables">表格（Table）</a></td>
<td>&#96;</td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#fenced-code-blocks">代码块（Fenced Code Block）</a></td>
<td><code>&#123; &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;age&quot;: 25&#125;</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#footnotes">脚注（Footnote）</a></td>
<td><code>Here&#39;s a sentence with a footnote. [^1][^1]: This is the footnote.</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#heading-ids">标题编号（Heading ID）</a></td>
<td><code>### My Great Heading &#123;#custom-id&#125;</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#definition-lists">定义列表（Definition List）</a></td>
<td><code>term: definition</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#strikethrough">删除线（Strikethrough）</a></td>
<td><code>~~The world is flat.~~</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#task-lists">任务列表（Task List）</a></td>
<td><code>- [x] Write the press release- [ ] Update the website- [ ] Contact the media</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#emoji">Emoji</a> (see also <a href="https://www.markdown.xyz/extended-syntax/#copying-and-pasting-emoji">Copying and Pasting Emoji</a>)</td>
<td><code>That is so funny! :joy:</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#highlight">Highlight</a></td>
<td><code>I need to highlight these ==very important words==.</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#subscript">Subscript</a></td>
<td><code>H~2~O</code></td>
</tr>
<tr>
<td><a href="https://www.markdown.xyz/extended-syntax/#superscript">Superscript</a></td>
<td><code>X^2^</code></td>
</tr>
</tbody></table>
<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>你可以 <a href="https://www.markdown.xyz/assets/markdown-cheat-sheet.md">下载此速查表的 Markdown 源码文件</a>，并在你自己的 Markdown 程序中使用。</p>
]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2025/01/20/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>前缀和与差分</title>
    <url>//2025/02/05/prefix-sum-and-difference/</url>
    <content><![CDATA[<p>要对原数组a[N]进行修改与查询，需要用到前缀和数组和差分数组<br>diff求和——&gt;a 求和——&gt;prefix</p>
<p>diff&lt;——a差分&lt;——prefix差分</p>
<h1 id="一维前缀和与差分"><a href="#一维前缀和与差分" class="headerlink" title="一维前缀和与差分"></a>一维前缀和与差分</h1><h2 id="前缀和"><a href="#前缀和" class="headerlink" title="前缀和"></a>前缀和</h2><p>前缀和可用于求指定区间的和</p>
<ol>
<li><strong>求出前缀和数组</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">前缀和数组的计算:prefix[i] = prefix[i<span class="number">-1</span>] + arr[i] (此处及下文写法i均从<span class="number">1</span>开始，以防止越界问题)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>得出区间和</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">由前缀和的可加性:sum[<span class="number">1</span>,k] + sum[k<span class="number">+1</span>,n] = sum[<span class="number">1</span>,n]</span><br><span class="line">得到sum[l,r] = sum[<span class="number">1</span>,r] - sum[<span class="number">1</span>,l<span class="number">-1</span>]，即sum=prefix[r]-prefix[l<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="差分"><a href="#差分" class="headerlink" title="差分"></a>差分</h2><p>差分可用于修改指定区间的值</p>
<ol>
<li><strong>求出差分数组</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">diff[i] = a[i] - a[i<span class="number">-1</span>]，它具有两个性质，一是可以通过求和得到a[i]，二是可以对后缀区间进行修改</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>对指定区间[l,r]的值进行修改</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">diff[l] += x;</span><br><span class="line">diff[r + <span class="number">1</span>] -= x;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><strong>求出diff的前缀和数组，得到修改后的数组</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">a[i] = a[i<span class="number">-1</span>] + diff[i]</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="二维前缀和与差分"><a href="#二维前缀和与差分" class="headerlink" title="二维前缀和与差分"></a>二维前缀和与差分</h1><p><strong>利用数形结合</strong>，前缀和可认为是数组向前拓展&#x2F;向前的射线涉及的区间的和，差分可认为是数组向后拓展&#x2F;向后的射线涉及的区间的修改</p>
<h2 id="前缀和-1"><a href="#前缀和-1" class="headerlink" title="前缀和"></a>前缀和</h2><ol>
<li><strong>求出前缀和数组</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; ++j) &#123;</span><br><span class="line">        prefix[i][j] = prefix[i<span class="number">-1</span>][j] + prefix[i][j<span class="number">-1</span>] - prefix[i<span class="number">-1</span>][j<span class="number">-1</span>] + arr[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>得出区间和</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">对于指定区间，设左上角为(x1, y1)，右下角为(x2, y2)</span><br><span class="line">sum = prefix[x2][y2] - prefix[x2][y1<span class="number">-1</span>] - prefix[x1<span class="number">-1</span>][y2] + prefix[x1<span class="number">-1</span>][y1<span class="number">-1</span>];</span><br></pre></td></tr></table></figure>

<h2 id="差分-1"><a href="#差分-1" class="headerlink" title="差分"></a>差分</h2><ol>
<li><strong>求出差分数组</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; ++j) &#123;</span><br><span class="line">        d[i][j] = arr[i][j] - arr[i][j<span class="number">-1</span>] - arr[i<span class="number">-1</span>][j] + arr[i<span class="number">-1</span>][j<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">&#123;d[i][j] += a[i][j];</span><br><span class="line">d[i + <span class="number">1</span>][j] -= a[i][j];</span><br><span class="line">d[i][j + <span class="number">1</span>] -= a[i][j];</span><br><span class="line">d[i + <span class="number">1</span>][j + <span class="number">1</span>] += a[i][j];&#125;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>对指定区间[(x1,y1),(x2,y2)]的值进行修改</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">d[x1][y1] += c;</span><br><span class="line">d[x2 + <span class="number">1</span>][y1] -= c;</span><br><span class="line">d[x1][y2 + <span class="number">1</span>] -= c;</span><br><span class="line">d[x2 + <span class="number">1</span>][y2 + <span class="number">1</span>] += c;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><strong>求出diff的前缀和数组，得到修改后的数组</strong></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; ++j) &#123;</span><br><span class="line">        arr[i][j] = arr[i<span class="number">-1</span>][j] + arr[i][j<span class="number">-1</span>] - arr[i<span class="number">-1</span>][j<span class="number">-1</span>] + d[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
</search>
